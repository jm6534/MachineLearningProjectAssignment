{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EOro3YNhQZs7"
      },
      "source": [
        "### Binary classification based on 3 layers neural network\n",
        "#### author: Kim Jeong Min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUDADJSwF4rN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a2068c3-7967-459b-eff2-ed957acdbbaa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KIFz7H8yQZs9"
      },
      "source": [
        "##### load images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3u1rh-hqQZs9",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pl\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime \n",
        "import csv\n",
        "import configparser\n",
        "import argparse\n",
        "import platform\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "import math\n",
        "import torch.utils.data as data_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Lf7hA6zQZtA",
        "colab": {}
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
        "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x) \n",
        "        \n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.sigmoid(self.t_conv2(x))\n",
        "                \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOUF8WPug-vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(images):\n",
        "  MEAN = 0\n",
        "  copyed = np.copy(images)\n",
        "  for img in copyed:\n",
        "    sigma = 0.01 * random.randint(1,5)\n",
        "    gaussian = np.random.normal(MEAN, sigma, (img.shape[0],img.shape[1]))\n",
        "    img = img + gaussian\n",
        "  return copyed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yhDyBWYVQZtC",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
        "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
        "                                transforms.ToTensor(),])\n",
        "\n",
        "\n",
        "#train_data_path = 'relative path of training data set'\n",
        "drive_prefix = 'data08/'\n",
        "train_data_path = drive_prefix + 'train.npy'\n",
        "train_set = np.load(train_data_path)\n",
        "# change the valuse of batch_size, num_workers for your program\n",
        "# if shuffle=True, the data reshuffled at every epoch \n",
        "noised = add_noise(train_set)\n",
        "\n",
        "# make 3d tensor object\n",
        "train_set = torch.from_numpy(train_set).unsqueeze(1)\n",
        "noised = torch.from_numpy(noised).unsqueeze(1)\n",
        "\n",
        "# set feature = noised, target = clean\n",
        "train_dataset = data_utils.TensorDataset(noised, train_set)\n",
        "loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)  \n",
        "\n",
        "test_data_path = drive_prefix + 'train.npy'\n",
        "test_set = np.load(test_data_path)\n",
        "test_set = torch.from_numpy(test_set).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qLLNfscxQZtE"
      },
      "source": [
        "##### load neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IopWIJWQZtE",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model = Autoencoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oA41bGU_QZtG"
      },
      "source": [
        "##### Set the flag for using cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J40z1SyRQZtH",
        "colab": {}
      },
      "source": [
        "bCuda = True\n",
        "if bCuda:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cCRBLDUhQZtI"
      },
      "source": [
        "##### optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uN1EBjMBQZtJ",
        "colab": {}
      },
      "source": [
        "LR = 0.003\n",
        "optimizer   = optim.SGD(model.parameters(), lr=LR, weight_decay = 1e-5, momentum=0.9)\n",
        "objective   = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5xc-HJgCQZtL"
      },
      "source": [
        "##### function for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjXKsB1aQZtL",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    # print('train the model at given epoch')\n",
        "    loss_train          = []\n",
        "    model.train()\n",
        "    for idx_batch, (data, target) in enumerate(loader_train):\n",
        "        if bCuda:\n",
        "            data, target    = data.cuda(), target.cuda()\n",
        "            \n",
        "        data, target    = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output  = model(data)\n",
        "        loss    = objective(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_train_batch    = loss.item() / len(data)\n",
        "        loss_train.append(loss_train_batch)\n",
        "        \n",
        "    loss_train_mean     = np.mean(loss_train)\n",
        "    loss_train_std      = np.std(loss_train)\n",
        "\n",
        "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAGLuK66QZtN"
      },
      "source": [
        "##### function for testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vs6mQq0xQZtO",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    return model(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uo7bMVqAQZtP"
      },
      "source": [
        "##### iteration for the epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QOV9Rt9JQZtQ",
        "outputId": "05b3578f-7480-4696-d4ed-a2defa330fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        }
      },
      "source": [
        "epoch = 1000\n",
        "loss_train_mean = list()\n",
        "loss_train_std = list()\n",
        "\n",
        "for e in range(epoch):\n",
        "        \n",
        "    result_train    = train()\n",
        "    \n",
        "    result_loss_train_mean = result_train['loss_train_mean']\n",
        "    result_loss_train_std =  result_train['loss_train_std']\n",
        "\n",
        "    loss_train_mean.append( result_loss_train_mean)\n",
        "    loss_train_std.append(  result_loss_train_std)\n",
        "    \n",
        "    epoch_str = '[epoch '+ '{:05d}'.format(e)+ ']  '\n",
        "    epoch_str += 'loss: '+ '(training)' + str(round(result_loss_train_mean, 15))\n",
        "    print(epoch_str)\n",
        "    \n",
        "    if e > epoch - 100 and accuracy_test[-1] > accuracy_test[-2] and accuracy_test[-1] > accuracy_test[-3]:\n",
        "        break;\n",
        "    \n",
        "    #adaptive learning rate\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = g['lr'] * 0.9985\n",
        "        \n",
        "        \n",
        "result_test = test()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 00000]  loss: (training)0.000661924021425\n",
            "[epoch 00001]  loss: (training)0.000628051349718\n",
            "[epoch 00002]  loss: (training)0.000620086627601\n",
            "[epoch 00003]  loss: (training)0.000614340348124\n",
            "[epoch 00004]  loss: (training)0.000610157225046\n",
            "[epoch 00005]  loss: (training)0.000611193973927\n",
            "[epoch 00006]  loss: (training)0.000607534732663\n",
            "[epoch 00007]  loss: (training)0.000607834676529\n",
            "[epoch 00008]  loss: (training)0.000606382531207\n",
            "[epoch 00009]  loss: (training)0.000606847562333\n",
            "[epoch 00010]  loss: (training)0.000606489398996\n",
            "[epoch 00011]  loss: (training)0.000607229529018\n",
            "[epoch 00012]  loss: (training)0.000607062910432\n",
            "[epoch 00013]  loss: (training)0.000604682530283\n",
            "[epoch 00014]  loss: (training)0.000603987857703\n",
            "[epoch 00015]  loss: (training)0.000602898179078\n",
            "[epoch 00016]  loss: (training)0.000605200998995\n",
            "[epoch 00017]  loss: (training)0.000606999782115\n",
            "[epoch 00018]  loss: (training)0.000604375554257\n",
            "[epoch 00019]  loss: (training)0.000602192781103\n",
            "[epoch 00020]  loss: (training)0.000603361935119\n",
            "[epoch 00021]  loss: (training)0.00060233841646\n",
            "[epoch 00022]  loss: (training)0.000602668714144\n",
            "[epoch 00023]  loss: (training)0.000600821399121\n",
            "[epoch 00024]  loss: (training)0.000602624530101\n",
            "[epoch 00025]  loss: (training)0.000602234620601\n",
            "[epoch 00026]  loss: (training)0.000599816824525\n",
            "[epoch 00027]  loss: (training)0.00060174589263\n",
            "[epoch 00028]  loss: (training)0.00059903659301\n",
            "[epoch 00029]  loss: (training)0.000597925572209\n",
            "[epoch 00030]  loss: (training)0.000598827253587\n",
            "[epoch 00031]  loss: (training)0.00059927259323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-6569a5b109dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult_train\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult_loss_train_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_train_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-ed8826921da8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss_train_batch\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f1Ri6ILMQZtU"
      },
      "source": [
        "##### plot result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JIPGvOdYQZtU",
        "colab": {}
      },
      "source": [
        "def print_history_graph(loss_history, loss_train_std):\n",
        "    #plt.plot(loss_history, color='#ff0000', label='Train Loss')\n",
        "    plt.errorbar(list(range(len(loss_history))), loss_history, yerr=loss_train_std, ecolor = '#ffcccc', color='#ff0000', label='Train Loss')\n",
        "    plt.plot(vloss_history, color='#0000ff', label='Validation Loss')\n",
        "    plt.legend(['Validation Loss','Train Loss'])\n",
        "    plt.title('Loss')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrXL9tkiQZtW",
        "colab": {}
      },
      "source": [
        "print_history_graph(loss_train_mean, loss_train_std)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}