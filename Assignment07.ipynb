{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification based on 3 layers neural network\n",
    "#### author: Kim Jeong Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime \n",
    "import csv\n",
    "import configparser\n",
    "import argparse\n",
    "import platform\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.ms = list()\n",
    "        \n",
    "        self.number_class   = num_classes\n",
    "\n",
    "        _size_image     = 100* 100\n",
    "        _num1           = 1024\n",
    "        _num2           = 256\n",
    "        _num3           = 64\n",
    "        \n",
    "        self.fc1        = nn.Linear(_size_image, _num1, bias=True)\n",
    "        self.fc2        = nn.Linear(_num1, _num2, bias=True)\n",
    "        self.fc3        = nn.Linear(_num2, _num3, bias=True)\n",
    "        self.fc4        = nn.Linear(_num3, num_classes, bias=True)\n",
    "        self.ms.append(self.fc1)\n",
    "        self.ms.append(self.fc2)\n",
    "        self.ms.append(self.fc3)\n",
    "        self.ms.append(self.fc4)\n",
    "\n",
    "        self.fc_layer1  = nn.Sequential(self.fc1, nn.ReLU())\n",
    "        self.fc_layer2  = nn.Sequential(self.fc2, nn.ReLU())\n",
    "        self.fc_layer3  = nn.Sequential(self.fc3, nn.ReLU())\n",
    "        self.fc_layer4  = nn.Sequential(self.fc4, nn.Sigmoid())\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2, self.fc_layer3, self.fc_layer4)\n",
    "        \n",
    "        self._initialize_weight()        \n",
    "        \n",
    "    def _initialize_weight(self):\n",
    "\n",
    "        for m in self.ms:\n",
    "            \n",
    "            n = m.in_features\n",
    "            m.weight.data.uniform_(- 1.0 / math.sqrt(n), 1.0 / math.sqrt(n))\n",
    "\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/train'\n",
    "trains_set = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "loader_train = torch.utils.data.DataLoader(trains_set, batch_size=64, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "test_data_path = 'horse-or-human/validation'\n",
    "test_set = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "loader_test = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=1)  \n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Linear(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the flag for using cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCuda = False\n",
    "if bCuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "optimizer   = optim.SGD(model.parameters(), lr=LR)\n",
    "objective   = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # print('train the model at given epoch')\n",
    "    loss_train          = []\n",
    "    acc_train           = []\n",
    "    model.train()\n",
    "    for idx_batch, (data, target) in enumerate(loader_train):\n",
    "        if bCuda:\n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "            \n",
    "        data, target    = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_batch    = loss.item() / len(data)\n",
    "        loss_train.append(loss_train_batch)\n",
    "        \n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        acc_train_batch   = 100. * float(correct) / len(data)\n",
    "        acc_train.append(acc_train_batch)\n",
    "        \n",
    "    loss_train_mean     = np.mean(loss_train)\n",
    "    loss_train_std      = np.std(loss_train)\n",
    "    acc_train_mean      = np.mean(acc_train)\n",
    "    acc_train_std       = np.std(acc_train)\n",
    "\n",
    "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std, 'acc_train_mean': acc_train_mean, 'acc_train_std': acc_train_std}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # print('test the model at given epoch')\n",
    "\n",
    "    accuracy_test   = []\n",
    "    loss_test       = 0\n",
    "    correct         = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx_batch, (data, target) in enumerate(loader_test):\n",
    "\n",
    "        if bCuda:\n",
    "        \n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss_test   += loss.item()\n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss_test       = loss_test / len(loader_test.dataset)\n",
    "    accuracy_test   = 100. * float(correct) / len(loader_test.dataset)\n",
    "\n",
    "    return {'loss_test': loss_test, 'accuracy_test': accuracy_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iteration for the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00000]  loss: (training)0.02377 (testing) 0.01083, accuracy: (training)50.91912 (testing) 50.0\n",
      "[epoch 00001]  loss: (training)0.0237 (testing) 0.01083, accuracy: (training)55.51471 (testing) 50.0\n",
      "[epoch 00002]  loss: (training)0.02365 (testing) 0.01083, accuracy: (training)53.30882 (testing) 50.0\n",
      "[epoch 00003]  loss: (training)0.02361 (testing) 0.01082, accuracy: (training)53.21691 (testing) 50.0\n",
      "[epoch 00004]  loss: (training)0.02357 (testing) 0.01082, accuracy: (training)53.21691 (testing) 50.0\n",
      "[epoch 00005]  loss: (training)0.02353 (testing) 0.01082, accuracy: (training)54.31985 (testing) 50.0\n",
      "[epoch 00006]  loss: (training)0.02349 (testing) 0.01081, accuracy: (training)54.87132 (testing) 50.0\n",
      "[epoch 00007]  loss: (training)0.02345 (testing) 0.01081, accuracy: (training)55.05515 (testing) 50.0\n",
      "[epoch 00008]  loss: (training)0.02341 (testing) 0.01081, accuracy: (training)55.69853 (testing) 50.0\n",
      "[epoch 00009]  loss: (training)0.02337 (testing) 0.01081, accuracy: (training)55.69853 (testing) 50.0\n",
      "[epoch 00010]  loss: (training)0.02333 (testing) 0.0108, accuracy: (training)55.51471 (testing) 50.0\n",
      "[epoch 00011]  loss: (training)0.02329 (testing) 0.0108, accuracy: (training)55.05515 (testing) 50.0\n",
      "[epoch 00012]  loss: (training)0.02325 (testing) 0.0108, accuracy: (training)54.87132 (testing) 50.0\n",
      "[epoch 00013]  loss: (training)0.02321 (testing) 0.01079, accuracy: (training)54.59559 (testing) 50.0\n",
      "[epoch 00014]  loss: (training)0.02317 (testing) 0.01079, accuracy: (training)54.77941 (testing) 50.0\n",
      "[epoch 00015]  loss: (training)0.02312 (testing) 0.01079, accuracy: (training)54.59559 (testing) 50.0\n",
      "[epoch 00016]  loss: (training)0.02308 (testing) 0.01079, accuracy: (training)54.59559 (testing) 50.0\n",
      "[epoch 00017]  loss: (training)0.02303 (testing) 0.01078, accuracy: (training)54.31985 (testing) 50.0\n",
      "[epoch 00018]  loss: (training)0.02298 (testing) 0.01079, accuracy: (training)54.50368 (testing) 50.0\n",
      "[epoch 00019]  loss: (training)0.02292 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00020]  loss: (training)0.02287 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00021]  loss: (training)0.02281 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00022]  loss: (training)0.02275 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00023]  loss: (training)0.0227 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00024]  loss: (training)0.02263 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00025]  loss: (training)0.02257 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00026]  loss: (training)0.0225 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00027]  loss: (training)0.02244 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00028]  loss: (training)0.02237 (testing) 0.01077, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00029]  loss: (training)0.0223 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00030]  loss: (training)0.02223 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00031]  loss: (training)0.02216 (testing) 0.01078, accuracy: (training)54.13603 (testing) 50.0\n",
      "[epoch 00032]  loss: (training)0.02209 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00033]  loss: (training)0.02201 (testing) 0.01078, accuracy: (training)54.22794 (testing) 50.0\n",
      "[epoch 00034]  loss: (training)0.02194 (testing) 0.01079, accuracy: (training)54.41176 (testing) 50.0\n",
      "[epoch 00035]  loss: (training)0.02187 (testing) 0.01078, accuracy: (training)54.41176 (testing) 50.0\n",
      "[epoch 00036]  loss: (training)0.0218 (testing) 0.01079, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00037]  loss: (training)0.02172 (testing) 0.01079, accuracy: (training)54.77941 (testing) 50.0\n",
      "[epoch 00038]  loss: (training)0.02165 (testing) 0.01079, accuracy: (training)54.87132 (testing) 50.0\n",
      "[epoch 00039]  loss: (training)0.02157 (testing) 0.01079, accuracy: (training)55.05515 (testing) 50.0\n",
      "[epoch 00040]  loss: (training)0.0215 (testing) 0.01079, accuracy: (training)55.23897 (testing) 50.0\n",
      "[epoch 00041]  loss: (training)0.02143 (testing) 0.01079, accuracy: (training)55.42279 (testing) 50.0\n",
      "[epoch 00042]  loss: (training)0.02135 (testing) 0.0108, accuracy: (training)55.51471 (testing) 50.0\n",
      "[epoch 00043]  loss: (training)0.02128 (testing) 0.01079, accuracy: (training)55.51471 (testing) 50.0\n",
      "[epoch 00044]  loss: (training)0.0212 (testing) 0.0108, accuracy: (training)55.60662 (testing) 50.0\n",
      "[epoch 00045]  loss: (training)0.02113 (testing) 0.01079, accuracy: (training)55.60662 (testing) 50.0\n",
      "[epoch 00046]  loss: (training)0.02106 (testing) 0.01079, accuracy: (training)55.60662 (testing) 50.0\n",
      "[epoch 00047]  loss: (training)0.02098 (testing) 0.01079, accuracy: (training)55.88235 (testing) 50.0\n",
      "[epoch 00048]  loss: (training)0.02091 (testing) 0.01078, accuracy: (training)56.34191 (testing) 50.0\n",
      "[epoch 00049]  loss: (training)0.02083 (testing) 0.01078, accuracy: (training)56.52574 (testing) 50.0\n",
      "[epoch 00050]  loss: (training)0.02075 (testing) 0.01077, accuracy: (training)56.98529 (testing) 50.0\n",
      "[epoch 00051]  loss: (training)0.02067 (testing) 0.01077, accuracy: (training)57.53676 (testing) 50.0\n",
      "[epoch 00052]  loss: (training)0.02058 (testing) 0.01076, accuracy: (training)57.44485 (testing) 50.0\n",
      "[epoch 00053]  loss: (training)0.0205 (testing) 0.01074, accuracy: (training)57.53676 (testing) 50.0\n",
      "[epoch 00054]  loss: (training)0.02041 (testing) 0.01074, accuracy: (training)58.18015 (testing) 50.0\n",
      "[epoch 00055]  loss: (training)0.02032 (testing) 0.01072, accuracy: (training)58.18015 (testing) 50.0\n",
      "[epoch 00056]  loss: (training)0.02023 (testing) 0.01073, accuracy: (training)58.82353 (testing) 50.0\n",
      "[epoch 00057]  loss: (training)0.02014 (testing) 0.01071, accuracy: (training)59.28309 (testing) 50.0\n",
      "[epoch 00058]  loss: (training)0.02004 (testing) 0.01071, accuracy: (training)60.01838 (testing) 50.0\n",
      "[epoch 00059]  loss: (training)0.01995 (testing) 0.01068, accuracy: (training)60.20221 (testing) 50.0\n",
      "[epoch 00060]  loss: (training)0.01984 (testing) 0.01069, accuracy: (training)61.21324 (testing) 50.0\n",
      "[epoch 00061]  loss: (training)0.01974 (testing) 0.01068, accuracy: (training)61.67279 (testing) 50.0\n",
      "[epoch 00062]  loss: (training)0.01963 (testing) 0.01066, accuracy: (training)60.9375 (testing) 50.0\n",
      "[epoch 00063]  loss: (training)0.01952 (testing) 0.01065, accuracy: (training)61.58088 (testing) 50.0\n",
      "[epoch 00064]  loss: (training)0.0194 (testing) 0.01063, accuracy: (training)61.48897 (testing) 50.0\n",
      "[epoch 00065]  loss: (training)0.01928 (testing) 0.01063, accuracy: (training)61.76471 (testing) 50.0\n",
      "[epoch 00066]  loss: (training)0.01917 (testing) 0.01062, accuracy: (training)62.68382 (testing) 50.0\n",
      "[epoch 00067]  loss: (training)0.01904 (testing) 0.01059, accuracy: (training)63.23529 (testing) 50.0\n",
      "[epoch 00068]  loss: (training)0.01891 (testing) 0.0106, accuracy: (training)63.69485 (testing) 50.0\n",
      "[epoch 00069]  loss: (training)0.01879 (testing) 0.01057, accuracy: (training)63.51103 (testing) 50.0\n",
      "[epoch 00070]  loss: (training)0.01865 (testing) 0.01057, accuracy: (training)64.0625 (testing) 50.39062\n",
      "[epoch 00071]  loss: (training)0.01853 (testing) 0.01054, accuracy: (training)63.87868 (testing) 50.78125\n",
      "[epoch 00072]  loss: (training)0.01839 (testing) 0.01053, accuracy: (training)63.60294 (testing) 50.78125\n",
      "[epoch 00073]  loss: (training)0.01825 (testing) 0.01051, accuracy: (training)63.69485 (testing) 51.17188\n",
      "[epoch 00074]  loss: (training)0.01812 (testing) 0.01049, accuracy: (training)64.15441 (testing) 51.17188\n",
      "[epoch 00075]  loss: (training)0.01798 (testing) 0.01048, accuracy: (training)64.43015 (testing) 51.17188\n",
      "[epoch 00076]  loss: (training)0.01785 (testing) 0.01046, accuracy: (training)64.61397 (testing) 51.17188\n",
      "[epoch 00077]  loss: (training)0.01771 (testing) 0.01043, accuracy: (training)64.98162 (testing) 51.5625\n",
      "[epoch 00078]  loss: (training)0.01757 (testing) 0.01042, accuracy: (training)65.625 (testing) 51.95312\n",
      "[epoch 00079]  loss: (training)0.01744 (testing) 0.01039, accuracy: (training)65.99265 (testing) 52.34375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00080]  loss: (training)0.0173 (testing) 0.01035, accuracy: (training)66.54412 (testing) 53.90625\n",
      "[epoch 00081]  loss: (training)0.01717 (testing) 0.01034, accuracy: (training)66.36029 (testing) 54.29688\n",
      "[epoch 00082]  loss: (training)0.01704 (testing) 0.01031, accuracy: (training)66.72794 (testing) 54.29688\n",
      "[epoch 00083]  loss: (training)0.01691 (testing) 0.01026, accuracy: (training)67.1875 (testing) 55.07812\n",
      "[epoch 00084]  loss: (training)0.01678 (testing) 0.01025, accuracy: (training)67.64706 (testing) 55.07812\n",
      "[epoch 00085]  loss: (training)0.01667 (testing) 0.0102, accuracy: (training)68.19853 (testing) 55.46875\n",
      "[epoch 00086]  loss: (training)0.01655 (testing) 0.01016, accuracy: (training)68.38235 (testing) 55.85938\n",
      "[epoch 00087]  loss: (training)0.01643 (testing) 0.01014, accuracy: (training)68.84191 (testing) 56.25\n",
      "[epoch 00088]  loss: (training)0.01632 (testing) 0.0101, accuracy: (training)69.02574 (testing) 56.25\n",
      "[epoch 00089]  loss: (training)0.01621 (testing) 0.01006, accuracy: (training)69.39338 (testing) 57.03125\n",
      "[epoch 00090]  loss: (training)0.0161 (testing) 0.01001, accuracy: (training)70.03676 (testing) 57.42188\n",
      "[epoch 00091]  loss: (training)0.016 (testing) 0.00997, accuracy: (training)70.77206 (testing) 57.8125\n",
      "[epoch 00092]  loss: (training)0.01591 (testing) 0.00993, accuracy: (training)71.04779 (testing) 58.59375\n",
      "[epoch 00093]  loss: (training)0.01581 (testing) 0.00989, accuracy: (training)71.32353 (testing) 58.98438\n",
      "[epoch 00094]  loss: (training)0.01572 (testing) 0.00984, accuracy: (training)71.41544 (testing) 58.98438\n",
      "[epoch 00095]  loss: (training)0.01563 (testing) 0.0098, accuracy: (training)71.875 (testing) 59.375\n",
      "[epoch 00096]  loss: (training)0.01555 (testing) 0.00974, accuracy: (training)72.51838 (testing) 59.76562\n",
      "[epoch 00097]  loss: (training)0.01547 (testing) 0.00971, accuracy: (training)73.06985 (testing) 60.15625\n",
      "[epoch 00098]  loss: (training)0.0154 (testing) 0.00965, accuracy: (training)73.4375 (testing) 60.9375\n",
      "[epoch 00099]  loss: (training)0.01532 (testing) 0.00961, accuracy: (training)73.89706 (testing) 62.10938\n",
      "[epoch 00100]  loss: (training)0.01525 (testing) 0.00956, accuracy: (training)74.17279 (testing) 63.28125\n",
      "[epoch 00101]  loss: (training)0.01519 (testing) 0.00951, accuracy: (training)74.35662 (testing) 64.0625\n",
      "[epoch 00102]  loss: (training)0.01512 (testing) 0.00948, accuracy: (training)74.54044 (testing) 64.84375\n",
      "[epoch 00103]  loss: (training)0.01506 (testing) 0.00943, accuracy: (training)74.81618 (testing) 66.01562\n",
      "[epoch 00104]  loss: (training)0.015 (testing) 0.00938, accuracy: (training)74.90809 (testing) 66.01562\n",
      "[epoch 00105]  loss: (training)0.01494 (testing) 0.00933, accuracy: (training)75.64338 (testing) 66.40625\n",
      "[epoch 00106]  loss: (training)0.01488 (testing) 0.0093, accuracy: (training)75.73529 (testing) 67.1875\n",
      "[epoch 00107]  loss: (training)0.01482 (testing) 0.00926, accuracy: (training)76.10294 (testing) 67.1875\n",
      "[epoch 00108]  loss: (training)0.01477 (testing) 0.0092, accuracy: (training)76.28676 (testing) 67.57812\n",
      "[epoch 00109]  loss: (training)0.01472 (testing) 0.00918, accuracy: (training)76.28676 (testing) 67.57812\n",
      "[epoch 00110]  loss: (training)0.01468 (testing) 0.00912, accuracy: (training)76.47059 (testing) 68.35938\n",
      "[epoch 00111]  loss: (training)0.01462 (testing) 0.00909, accuracy: (training)76.65441 (testing) 68.35938\n",
      "[epoch 00112]  loss: (training)0.01458 (testing) 0.00904, accuracy: (training)77.11397 (testing) 69.92188\n",
      "[epoch 00113]  loss: (training)0.01454 (testing) 0.00899, accuracy: (training)77.48162 (testing) 70.70312\n",
      "[epoch 00114]  loss: (training)0.01449 (testing) 0.00896, accuracy: (training)77.75735 (testing) 71.09375\n",
      "[epoch 00115]  loss: (training)0.01445 (testing) 0.00891, accuracy: (training)78.125 (testing) 71.48438\n",
      "[epoch 00116]  loss: (training)0.01441 (testing) 0.00887, accuracy: (training)78.67647 (testing) 71.875\n",
      "[epoch 00117]  loss: (training)0.01437 (testing) 0.00883, accuracy: (training)78.76838 (testing) 72.65625\n",
      "[epoch 00118]  loss: (training)0.01434 (testing) 0.00879, accuracy: (training)79.13603 (testing) 73.4375\n",
      "[epoch 00119]  loss: (training)0.0143 (testing) 0.00874, accuracy: (training)79.31985 (testing) 73.82812\n",
      "[epoch 00120]  loss: (training)0.01426 (testing) 0.00871, accuracy: (training)79.50368 (testing) 74.21875\n",
      "[epoch 00121]  loss: (training)0.01423 (testing) 0.00867, accuracy: (training)79.50368 (testing) 74.60938\n",
      "[epoch 00122]  loss: (training)0.0142 (testing) 0.00863, accuracy: (training)79.59559 (testing) 75.0\n",
      "[epoch 00123]  loss: (training)0.01417 (testing) 0.00859, accuracy: (training)79.6875 (testing) 75.0\n",
      "[epoch 00124]  loss: (training)0.01414 (testing) 0.00857, accuracy: (training)79.77941 (testing) 75.0\n",
      "[epoch 00125]  loss: (training)0.01411 (testing) 0.00852, accuracy: (training)79.87132 (testing) 75.0\n",
      "[epoch 00126]  loss: (training)0.01408 (testing) 0.00849, accuracy: (training)80.42279 (testing) 75.39062\n",
      "[epoch 00127]  loss: (training)0.01405 (testing) 0.00846, accuracy: (training)80.42279 (testing) 75.39062\n",
      "[epoch 00128]  loss: (training)0.01402 (testing) 0.00842, accuracy: (training)80.60662 (testing) 76.17188\n",
      "[epoch 00129]  loss: (training)0.01399 (testing) 0.00839, accuracy: (training)80.60662 (testing) 76.17188\n",
      "[epoch 00130]  loss: (training)0.01397 (testing) 0.00836, accuracy: (training)80.88235 (testing) 76.17188\n",
      "[epoch 00131]  loss: (training)0.01394 (testing) 0.00833, accuracy: (training)80.97426 (testing) 76.17188\n",
      "[epoch 00132]  loss: (training)0.01392 (testing) 0.0083, accuracy: (training)80.88235 (testing) 77.34375\n",
      "[epoch 00133]  loss: (training)0.01389 (testing) 0.00827, accuracy: (training)81.06618 (testing) 78.51562\n",
      "[epoch 00134]  loss: (training)0.01387 (testing) 0.00825, accuracy: (training)81.25 (testing) 78.51562\n",
      "[epoch 00135]  loss: (training)0.01384 (testing) 0.00821, accuracy: (training)81.34191 (testing) 78.90625\n",
      "[epoch 00136]  loss: (training)0.01382 (testing) 0.00819, accuracy: (training)81.43382 (testing) 78.90625\n",
      "[epoch 00137]  loss: (training)0.0138 (testing) 0.00816, accuracy: (training)81.61765 (testing) 78.90625\n",
      "[epoch 00138]  loss: (training)0.01378 (testing) 0.00814, accuracy: (training)81.70956 (testing) 79.29688\n",
      "[epoch 00139]  loss: (training)0.01375 (testing) 0.00811, accuracy: (training)81.61765 (testing) 79.29688\n",
      "[epoch 00140]  loss: (training)0.01373 (testing) 0.00809, accuracy: (training)81.80147 (testing) 79.29688\n",
      "[epoch 00141]  loss: (training)0.01371 (testing) 0.00807, accuracy: (training)81.98529 (testing) 79.6875\n",
      "[epoch 00142]  loss: (training)0.01369 (testing) 0.00804, accuracy: (training)82.07721 (testing) 80.07812\n",
      "[epoch 00143]  loss: (training)0.01367 (testing) 0.00802, accuracy: (training)82.07721 (testing) 80.85938\n",
      "[epoch 00144]  loss: (training)0.01365 (testing) 0.008, accuracy: (training)82.16912 (testing) 80.85938\n",
      "[epoch 00145]  loss: (training)0.01362 (testing) 0.00798, accuracy: (training)82.16912 (testing) 80.85938\n",
      "[epoch 00146]  loss: (training)0.0136 (testing) 0.00796, accuracy: (training)82.16912 (testing) 81.25\n",
      "[epoch 00147]  loss: (training)0.01358 (testing) 0.00793, accuracy: (training)82.35294 (testing) 81.25\n",
      "[epoch 00148]  loss: (training)0.01356 (testing) 0.00792, accuracy: (training)82.53676 (testing) 81.25\n",
      "[epoch 00149]  loss: (training)0.01354 (testing) 0.0079, accuracy: (training)82.62868 (testing) 81.25\n",
      "[epoch 00150]  loss: (training)0.01352 (testing) 0.00788, accuracy: (training)82.99632 (testing) 81.25\n",
      "[epoch 00151]  loss: (training)0.0135 (testing) 0.00786, accuracy: (training)83.08824 (testing) 81.25\n",
      "[epoch 00152]  loss: (training)0.01348 (testing) 0.00784, accuracy: (training)83.27206 (testing) 81.25\n",
      "[epoch 00153]  loss: (training)0.01346 (testing) 0.00783, accuracy: (training)83.45588 (testing) 81.25\n",
      "[epoch 00154]  loss: (training)0.01344 (testing) 0.00781, accuracy: (training)83.63971 (testing) 81.25\n",
      "[epoch 00155]  loss: (training)0.01342 (testing) 0.00779, accuracy: (training)83.73162 (testing) 81.25\n",
      "[epoch 00156]  loss: (training)0.0134 (testing) 0.00778, accuracy: (training)83.73162 (testing) 81.25\n",
      "[epoch 00157]  loss: (training)0.01338 (testing) 0.00776, accuracy: (training)83.73162 (testing) 81.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00158]  loss: (training)0.01336 (testing) 0.00774, accuracy: (training)83.82353 (testing) 81.25\n",
      "[epoch 00159]  loss: (training)0.01334 (testing) 0.00773, accuracy: (training)83.91544 (testing) 81.25\n",
      "[epoch 00160]  loss: (training)0.01333 (testing) 0.00772, accuracy: (training)84.00735 (testing) 81.25\n",
      "[epoch 00161]  loss: (training)0.01331 (testing) 0.0077, accuracy: (training)84.00735 (testing) 81.25\n",
      "[epoch 00162]  loss: (training)0.01329 (testing) 0.00769, accuracy: (training)84.00735 (testing) 81.25\n",
      "[epoch 00163]  loss: (training)0.01327 (testing) 0.00767, accuracy: (training)84.09926 (testing) 81.25\n",
      "[epoch 00164]  loss: (training)0.01325 (testing) 0.00766, accuracy: (training)84.09926 (testing) 81.64062\n",
      "[epoch 00165]  loss: (training)0.01323 (testing) 0.00765, accuracy: (training)84.375 (testing) 81.64062\n",
      "[epoch 00166]  loss: (training)0.01321 (testing) 0.00763, accuracy: (training)84.375 (testing) 82.03125\n",
      "[epoch 00167]  loss: (training)0.01319 (testing) 0.00762, accuracy: (training)84.46691 (testing) 82.03125\n",
      "[epoch 00168]  loss: (training)0.01317 (testing) 0.00761, accuracy: (training)84.74265 (testing) 82.03125\n",
      "[epoch 00169]  loss: (training)0.01316 (testing) 0.0076, accuracy: (training)84.92647 (testing) 82.03125\n",
      "[epoch 00170]  loss: (training)0.01314 (testing) 0.00759, accuracy: (training)85.11029 (testing) 82.03125\n",
      "[epoch 00171]  loss: (training)0.01312 (testing) 0.00757, accuracy: (training)85.56985 (testing) 82.03125\n",
      "[epoch 00172]  loss: (training)0.0131 (testing) 0.00756, accuracy: (training)85.56985 (testing) 82.03125\n",
      "[epoch 00173]  loss: (training)0.01308 (testing) 0.00755, accuracy: (training)85.66176 (testing) 82.03125\n",
      "[epoch 00174]  loss: (training)0.01307 (testing) 0.00754, accuracy: (training)85.66176 (testing) 82.42188\n",
      "[epoch 00175]  loss: (training)0.01305 (testing) 0.00753, accuracy: (training)86.02941 (testing) 82.8125\n",
      "[epoch 00176]  loss: (training)0.01303 (testing) 0.00752, accuracy: (training)86.39706 (testing) 82.8125\n",
      "[epoch 00177]  loss: (training)0.01302 (testing) 0.00751, accuracy: (training)86.67279 (testing) 82.8125\n",
      "[epoch 00178]  loss: (training)0.013 (testing) 0.0075, accuracy: (training)86.76471 (testing) 82.8125\n",
      "[epoch 00179]  loss: (training)0.01298 (testing) 0.00748, accuracy: (training)86.94853 (testing) 82.8125\n",
      "[epoch 00180]  loss: (training)0.01297 (testing) 0.00748, accuracy: (training)86.94853 (testing) 82.8125\n",
      "[epoch 00181]  loss: (training)0.01295 (testing) 0.00747, accuracy: (training)86.94853 (testing) 82.8125\n",
      "[epoch 00182]  loss: (training)0.01294 (testing) 0.00746, accuracy: (training)87.04044 (testing) 82.8125\n",
      "[epoch 00183]  loss: (training)0.01292 (testing) 0.00745, accuracy: (training)87.13235 (testing) 82.8125\n",
      "[epoch 00184]  loss: (training)0.01291 (testing) 0.00744, accuracy: (training)87.22426 (testing) 82.8125\n",
      "[epoch 00185]  loss: (training)0.0129 (testing) 0.00743, accuracy: (training)87.22426 (testing) 82.8125\n",
      "[epoch 00186]  loss: (training)0.01288 (testing) 0.00742, accuracy: (training)87.31618 (testing) 82.8125\n",
      "[epoch 00187]  loss: (training)0.01287 (testing) 0.00741, accuracy: (training)87.40809 (testing) 82.8125\n",
      "[epoch 00188]  loss: (training)0.01286 (testing) 0.0074, accuracy: (training)87.68382 (testing) 82.8125\n",
      "[epoch 00189]  loss: (training)0.01284 (testing) 0.0074, accuracy: (training)87.77574 (testing) 83.20312\n",
      "[epoch 00190]  loss: (training)0.01283 (testing) 0.00739, accuracy: (training)87.86765 (testing) 83.59375\n",
      "[epoch 00191]  loss: (training)0.01282 (testing) 0.00738, accuracy: (training)87.86765 (testing) 83.59375\n",
      "[epoch 00192]  loss: (training)0.01281 (testing) 0.00737, accuracy: (training)87.95956 (testing) 83.98438\n",
      "[epoch 00193]  loss: (training)0.01279 (testing) 0.00737, accuracy: (training)88.05147 (testing) 83.98438\n",
      "[epoch 00194]  loss: (training)0.01278 (testing) 0.00736, accuracy: (training)88.41912 (testing) 83.98438\n",
      "[epoch 00195]  loss: (training)0.01277 (testing) 0.00735, accuracy: (training)88.51103 (testing) 83.98438\n",
      "[epoch 00196]  loss: (training)0.01276 (testing) 0.00735, accuracy: (training)88.51103 (testing) 83.98438\n",
      "[epoch 00197]  loss: (training)0.01275 (testing) 0.00734, accuracy: (training)88.60294 (testing) 83.98438\n",
      "[epoch 00198]  loss: (training)0.01274 (testing) 0.00733, accuracy: (training)88.87868 (testing) 83.98438\n",
      "[epoch 00199]  loss: (training)0.01273 (testing) 0.00733, accuracy: (training)88.97059 (testing) 83.98438\n",
      "[epoch 00200]  loss: (training)0.01272 (testing) 0.00732, accuracy: (training)88.97059 (testing) 83.98438\n",
      "[epoch 00201]  loss: (training)0.01271 (testing) 0.00732, accuracy: (training)88.87868 (testing) 83.98438\n",
      "[epoch 00202]  loss: (training)0.0127 (testing) 0.00731, accuracy: (training)88.97059 (testing) 83.98438\n",
      "[epoch 00203]  loss: (training)0.0127 (testing) 0.00731, accuracy: (training)89.15441 (testing) 83.98438\n",
      "[epoch 00204]  loss: (training)0.01269 (testing) 0.0073, accuracy: (training)89.15441 (testing) 83.98438\n",
      "[epoch 00205]  loss: (training)0.01268 (testing) 0.0073, accuracy: (training)89.15441 (testing) 83.98438\n",
      "[epoch 00206]  loss: (training)0.01267 (testing) 0.00729, accuracy: (training)89.15441 (testing) 83.98438\n",
      "[epoch 00207]  loss: (training)0.01266 (testing) 0.00729, accuracy: (training)89.15441 (testing) 83.98438\n",
      "[epoch 00208]  loss: (training)0.01266 (testing) 0.00728, accuracy: (training)89.33824 (testing) 83.98438\n",
      "[epoch 00209]  loss: (training)0.01265 (testing) 0.00728, accuracy: (training)89.43015 (testing) 83.98438\n",
      "[epoch 00210]  loss: (training)0.01264 (testing) 0.00728, accuracy: (training)89.70588 (testing) 84.375\n",
      "[epoch 00211]  loss: (training)0.01263 (testing) 0.00727, accuracy: (training)89.79779 (testing) 84.375\n",
      "[epoch 00212]  loss: (training)0.01263 (testing) 0.00727, accuracy: (training)89.79779 (testing) 84.375\n",
      "[epoch 00213]  loss: (training)0.01262 (testing) 0.00727, accuracy: (training)89.88971 (testing) 84.375\n",
      "[epoch 00214]  loss: (training)0.01262 (testing) 0.00726, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00215]  loss: (training)0.01261 (testing) 0.00726, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00216]  loss: (training)0.0126 (testing) 0.00726, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00217]  loss: (training)0.0126 (testing) 0.00725, accuracy: (training)90.07353 (testing) 84.375\n",
      "[epoch 00218]  loss: (training)0.01259 (testing) 0.00725, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00219]  loss: (training)0.01258 (testing) 0.00725, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00220]  loss: (training)0.01258 (testing) 0.00725, accuracy: (training)89.98162 (testing) 84.375\n",
      "[epoch 00221]  loss: (training)0.01257 (testing) 0.00725, accuracy: (training)89.98162 (testing) 83.98438\n",
      "[epoch 00222]  loss: (training)0.01257 (testing) 0.00724, accuracy: (training)89.98162 (testing) 83.98438\n",
      "[epoch 00223]  loss: (training)0.01256 (testing) 0.00724, accuracy: (training)89.98162 (testing) 83.98438\n",
      "[epoch 00224]  loss: (training)0.01256 (testing) 0.00724, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00225]  loss: (training)0.01255 (testing) 0.00724, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00226]  loss: (training)0.01255 (testing) 0.00724, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00227]  loss: (training)0.01254 (testing) 0.00724, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00228]  loss: (training)0.01253 (testing) 0.00723, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00229]  loss: (training)0.01253 (testing) 0.00723, accuracy: (training)90.07353 (testing) 83.98438\n",
      "[epoch 00230]  loss: (training)0.01252 (testing) 0.00723, accuracy: (training)90.16544 (testing) 83.98438\n",
      "[epoch 00231]  loss: (training)0.01252 (testing) 0.00723, accuracy: (training)90.34926 (testing) 83.98438\n",
      "[epoch 00232]  loss: (training)0.01251 (testing) 0.00723, accuracy: (training)90.44118 (testing) 83.98438\n",
      "[epoch 00233]  loss: (training)0.01251 (testing) 0.00723, accuracy: (training)90.44118 (testing) 83.98438\n",
      "[epoch 00234]  loss: (training)0.01251 (testing) 0.00723, accuracy: (training)90.44118 (testing) 83.98438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00235]  loss: (training)0.0125 (testing) 0.00723, accuracy: (training)90.44118 (testing) 83.98438\n",
      "[epoch 00236]  loss: (training)0.0125 (testing) 0.00723, accuracy: (training)90.44118 (testing) 83.98438\n",
      "[epoch 00237]  loss: (training)0.01249 (testing) 0.00723, accuracy: (training)90.44118 (testing) 84.375\n",
      "[epoch 00238]  loss: (training)0.01249 (testing) 0.00723, accuracy: (training)90.44118 (testing) 84.375\n",
      "[epoch 00239]  loss: (training)0.01248 (testing) 0.00723, accuracy: (training)90.625 (testing) 84.375\n",
      "[epoch 00240]  loss: (training)0.01248 (testing) 0.00723, accuracy: (training)90.625 (testing) 84.375\n",
      "[epoch 00241]  loss: (training)0.01247 (testing) 0.00722, accuracy: (training)90.71691 (testing) 84.375\n",
      "[epoch 00242]  loss: (training)0.01247 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00243]  loss: (training)0.01246 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00244]  loss: (training)0.01246 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00245]  loss: (training)0.01245 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00246]  loss: (training)0.01245 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00247]  loss: (training)0.01244 (testing) 0.00722, accuracy: (training)90.71691 (testing) 83.98438\n",
      "[epoch 00248]  loss: (training)0.01244 (testing) 0.00722, accuracy: (training)90.71691 (testing) 84.375\n",
      "[epoch 00249]  loss: (training)0.01243 (testing) 0.00722, accuracy: (training)90.80882 (testing) 84.375\n",
      "[epoch 00250]  loss: (training)0.01243 (testing) 0.00722, accuracy: (training)90.80882 (testing) 84.76562\n",
      "[epoch 00251]  loss: (training)0.01243 (testing) 0.00722, accuracy: (training)90.80882 (testing) 84.76562\n",
      "[epoch 00252]  loss: (training)0.01242 (testing) 0.00722, accuracy: (training)90.80882 (testing) 84.76562\n",
      "[epoch 00253]  loss: (training)0.01242 (testing) 0.00722, accuracy: (training)90.80882 (testing) 84.76562\n",
      "[epoch 00254]  loss: (training)0.01241 (testing) 0.00722, accuracy: (training)90.80882 (testing) 85.15625\n",
      "[epoch 00255]  loss: (training)0.01241 (testing) 0.00721, accuracy: (training)90.90074 (testing) 85.15625\n",
      "[epoch 00256]  loss: (training)0.0124 (testing) 0.00721, accuracy: (training)90.99265 (testing) 85.15625\n",
      "[epoch 00257]  loss: (training)0.0124 (testing) 0.00721, accuracy: (training)90.99265 (testing) 85.15625\n",
      "[epoch 00258]  loss: (training)0.01239 (testing) 0.00721, accuracy: (training)91.08456 (testing) 85.15625\n",
      "[epoch 00259]  loss: (training)0.01239 (testing) 0.00721, accuracy: (training)91.26838 (testing) 85.15625\n",
      "[epoch 00260]  loss: (training)0.01238 (testing) 0.00721, accuracy: (training)91.26838 (testing) 85.15625\n",
      "[epoch 00261]  loss: (training)0.01238 (testing) 0.00721, accuracy: (training)91.26838 (testing) 85.15625\n",
      "[epoch 00262]  loss: (training)0.01237 (testing) 0.00721, accuracy: (training)91.36029 (testing) 85.15625\n",
      "[epoch 00263]  loss: (training)0.01237 (testing) 0.00721, accuracy: (training)91.63603 (testing) 85.15625\n",
      "[epoch 00264]  loss: (training)0.01236 (testing) 0.00721, accuracy: (training)91.72794 (testing) 85.15625\n",
      "[epoch 00265]  loss: (training)0.01236 (testing) 0.00721, accuracy: (training)91.81985 (testing) 85.15625\n",
      "[epoch 00266]  loss: (training)0.01235 (testing) 0.00721, accuracy: (training)91.91176 (testing) 85.15625\n",
      "[epoch 00267]  loss: (training)0.01235 (testing) 0.0072, accuracy: (training)91.91176 (testing) 85.15625\n",
      "[epoch 00268]  loss: (training)0.01234 (testing) 0.0072, accuracy: (training)91.91176 (testing) 85.15625\n",
      "[epoch 00269]  loss: (training)0.01234 (testing) 0.0072, accuracy: (training)91.91176 (testing) 85.15625\n",
      "[epoch 00270]  loss: (training)0.01233 (testing) 0.0072, accuracy: (training)91.91176 (testing) 85.15625\n",
      "[epoch 00271]  loss: (training)0.01233 (testing) 0.0072, accuracy: (training)92.00368 (testing) 85.15625\n",
      "[epoch 00272]  loss: (training)0.01232 (testing) 0.0072, accuracy: (training)92.00368 (testing) 85.15625\n",
      "[epoch 00273]  loss: (training)0.01232 (testing) 0.0072, accuracy: (training)92.09559 (testing) 85.15625\n",
      "[epoch 00274]  loss: (training)0.01231 (testing) 0.0072, accuracy: (training)92.1875 (testing) 85.15625\n",
      "[epoch 00275]  loss: (training)0.01231 (testing) 0.0072, accuracy: (training)92.1875 (testing) 85.15625\n",
      "[epoch 00276]  loss: (training)0.0123 (testing) 0.0072, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00277]  loss: (training)0.0123 (testing) 0.0072, accuracy: (training)92.1875 (testing) 85.15625\n",
      "[epoch 00278]  loss: (training)0.01229 (testing) 0.0072, accuracy: (training)92.1875 (testing) 85.15625\n",
      "[epoch 00279]  loss: (training)0.01229 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00280]  loss: (training)0.01228 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00281]  loss: (training)0.01228 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00282]  loss: (training)0.01227 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00283]  loss: (training)0.01227 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00284]  loss: (training)0.01226 (testing) 0.00719, accuracy: (training)92.27941 (testing) 85.15625\n",
      "[epoch 00285]  loss: (training)0.01226 (testing) 0.00719, accuracy: (training)92.37132 (testing) 85.15625\n",
      "[epoch 00286]  loss: (training)0.01226 (testing) 0.00719, accuracy: (training)92.46324 (testing) 85.15625\n",
      "[epoch 00287]  loss: (training)0.01225 (testing) 0.00719, accuracy: (training)92.55515 (testing) 85.15625\n",
      "[epoch 00288]  loss: (training)0.01225 (testing) 0.00719, accuracy: (training)92.55515 (testing) 85.15625\n",
      "[epoch 00289]  loss: (training)0.01224 (testing) 0.00719, accuracy: (training)92.55515 (testing) 85.15625\n",
      "[epoch 00290]  loss: (training)0.01224 (testing) 0.00719, accuracy: (training)92.55515 (testing) 85.15625\n",
      "[epoch 00291]  loss: (training)0.01223 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00292]  loss: (training)0.01223 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00293]  loss: (training)0.01223 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00294]  loss: (training)0.01222 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00295]  loss: (training)0.01222 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00296]  loss: (training)0.01221 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00297]  loss: (training)0.01221 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00298]  loss: (training)0.01221 (testing) 0.00718, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00299]  loss: (training)0.0122 (testing) 0.00718, accuracy: (training)92.73897 (testing) 85.54688\n",
      "[epoch 00300]  loss: (training)0.0122 (testing) 0.00717, accuracy: (training)92.73897 (testing) 85.54688\n",
      "[epoch 00301]  loss: (training)0.01219 (testing) 0.00717, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00302]  loss: (training)0.01219 (testing) 0.00717, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00303]  loss: (training)0.01219 (testing) 0.00717, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00304]  loss: (training)0.01218 (testing) 0.00717, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00305]  loss: (training)0.01218 (testing) 0.00717, accuracy: (training)92.55515 (testing) 85.54688\n",
      "[epoch 00306]  loss: (training)0.01218 (testing) 0.00717, accuracy: (training)92.55515 (testing) 85.54688\n",
      "[epoch 00307]  loss: (training)0.01217 (testing) 0.00716, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00308]  loss: (training)0.01217 (testing) 0.00716, accuracy: (training)92.64706 (testing) 85.54688\n",
      "[epoch 00309]  loss: (training)0.01217 (testing) 0.00716, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00310]  loss: (training)0.01216 (testing) 0.00716, accuracy: (training)92.64706 (testing) 85.15625\n",
      "[epoch 00311]  loss: (training)0.01216 (testing) 0.00716, accuracy: (training)92.64706 (testing) 85.15625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00312]  loss: (training)0.01216 (testing) 0.00715, accuracy: (training)92.73897 (testing) 85.15625\n",
      "[epoch 00313]  loss: (training)0.01215 (testing) 0.00715, accuracy: (training)92.73897 (testing) 85.15625\n",
      "[epoch 00314]  loss: (training)0.01215 (testing) 0.00715, accuracy: (training)92.83088 (testing) 85.15625\n",
      "[epoch 00315]  loss: (training)0.01214 (testing) 0.00715, accuracy: (training)92.83088 (testing) 85.15625\n",
      "[epoch 00316]  loss: (training)0.01214 (testing) 0.00715, accuracy: (training)92.83088 (testing) 85.15625\n",
      "[epoch 00317]  loss: (training)0.01214 (testing) 0.00715, accuracy: (training)92.83088 (testing) 85.15625\n",
      "[epoch 00318]  loss: (training)0.01213 (testing) 0.00714, accuracy: (training)92.83088 (testing) 85.15625\n",
      "[epoch 00319]  loss: (training)0.01213 (testing) 0.00714, accuracy: (training)92.92279 (testing) 85.15625\n",
      "[epoch 00320]  loss: (training)0.01213 (testing) 0.00714, accuracy: (training)93.01471 (testing) 85.15625\n",
      "[epoch 00321]  loss: (training)0.01212 (testing) 0.00714, accuracy: (training)93.01471 (testing) 85.15625\n",
      "[epoch 00322]  loss: (training)0.01212 (testing) 0.00714, accuracy: (training)93.10662 (testing) 85.15625\n",
      "[epoch 00323]  loss: (training)0.01212 (testing) 0.00713, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00324]  loss: (training)0.01211 (testing) 0.00713, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00325]  loss: (training)0.01211 (testing) 0.00713, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00326]  loss: (training)0.01211 (testing) 0.00713, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00327]  loss: (training)0.0121 (testing) 0.00713, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00328]  loss: (training)0.0121 (testing) 0.00712, accuracy: (training)93.19853 (testing) 85.15625\n",
      "[epoch 00329]  loss: (training)0.0121 (testing) 0.00712, accuracy: (training)93.29044 (testing) 85.15625\n",
      "[epoch 00330]  loss: (training)0.01209 (testing) 0.00712, accuracy: (training)93.29044 (testing) 85.15625\n",
      "[epoch 00331]  loss: (training)0.01209 (testing) 0.00712, accuracy: (training)93.29044 (testing) 85.15625\n",
      "[epoch 00332]  loss: (training)0.01209 (testing) 0.00712, accuracy: (training)93.38235 (testing) 85.15625\n",
      "[epoch 00333]  loss: (training)0.01208 (testing) 0.00711, accuracy: (training)93.38235 (testing) 85.15625\n",
      "[epoch 00334]  loss: (training)0.01208 (testing) 0.00711, accuracy: (training)93.47426 (testing) 85.15625\n",
      "[epoch 00335]  loss: (training)0.01208 (testing) 0.00711, accuracy: (training)93.56618 (testing) 85.15625\n",
      "[epoch 00336]  loss: (training)0.01207 (testing) 0.00711, accuracy: (training)93.56618 (testing) 85.15625\n",
      "[epoch 00337]  loss: (training)0.01207 (testing) 0.0071, accuracy: (training)93.47426 (testing) 85.15625\n",
      "[epoch 00338]  loss: (training)0.01206 (testing) 0.0071, accuracy: (training)93.56618 (testing) 85.15625\n",
      "[epoch 00339]  loss: (training)0.01206 (testing) 0.0071, accuracy: (training)93.56618 (testing) 85.15625\n",
      "[epoch 00340]  loss: (training)0.01206 (testing) 0.0071, accuracy: (training)93.56618 (testing) 85.15625\n",
      "[epoch 00341]  loss: (training)0.01205 (testing) 0.00709, accuracy: (training)93.65809 (testing) 85.54688\n",
      "[epoch 00342]  loss: (training)0.01205 (testing) 0.00709, accuracy: (training)93.65809 (testing) 85.54688\n",
      "[epoch 00343]  loss: (training)0.01205 (testing) 0.00709, accuracy: (training)93.65809 (testing) 85.54688\n",
      "[epoch 00344]  loss: (training)0.01204 (testing) 0.00709, accuracy: (training)93.65809 (testing) 85.54688\n",
      "[epoch 00345]  loss: (training)0.01204 (testing) 0.00708, accuracy: (training)93.65809 (testing) 85.9375\n",
      "[epoch 00346]  loss: (training)0.01204 (testing) 0.00708, accuracy: (training)93.65809 (testing) 85.9375\n",
      "[epoch 00347]  loss: (training)0.01203 (testing) 0.00708, accuracy: (training)93.75 (testing) 85.9375\n",
      "[epoch 00348]  loss: (training)0.01203 (testing) 0.00707, accuracy: (training)93.75 (testing) 85.9375\n",
      "[epoch 00349]  loss: (training)0.01203 (testing) 0.00707, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00350]  loss: (training)0.01202 (testing) 0.00707, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00351]  loss: (training)0.01202 (testing) 0.00707, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00352]  loss: (training)0.01202 (testing) 0.00706, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00353]  loss: (training)0.01201 (testing) 0.00706, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00354]  loss: (training)0.01201 (testing) 0.00706, accuracy: (training)93.84191 (testing) 85.9375\n",
      "[epoch 00355]  loss: (training)0.01201 (testing) 0.00705, accuracy: (training)94.02574 (testing) 85.9375\n",
      "[epoch 00356]  loss: (training)0.012 (testing) 0.00705, accuracy: (training)94.02574 (testing) 85.9375\n",
      "[epoch 00357]  loss: (training)0.012 (testing) 0.00705, accuracy: (training)94.02574 (testing) 85.9375\n",
      "[epoch 00358]  loss: (training)0.012 (testing) 0.00704, accuracy: (training)94.02574 (testing) 85.9375\n",
      "[epoch 00359]  loss: (training)0.01199 (testing) 0.00704, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00360]  loss: (training)0.01199 (testing) 0.00704, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00361]  loss: (training)0.01199 (testing) 0.00703, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00362]  loss: (training)0.01198 (testing) 0.00703, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00363]  loss: (training)0.01198 (testing) 0.00703, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00364]  loss: (training)0.01198 (testing) 0.00702, accuracy: (training)93.93382 (testing) 86.32812\n",
      "[epoch 00365]  loss: (training)0.01197 (testing) 0.00702, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00366]  loss: (training)0.01197 (testing) 0.00702, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00367]  loss: (training)0.01197 (testing) 0.00701, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00368]  loss: (training)0.01196 (testing) 0.00701, accuracy: (training)94.02574 (testing) 86.32812\n",
      "[epoch 00369]  loss: (training)0.01196 (testing) 0.00701, accuracy: (training)94.11765 (testing) 86.32812\n",
      "[epoch 00370]  loss: (training)0.01195 (testing) 0.007, accuracy: (training)94.11765 (testing) 86.32812\n",
      "[epoch 00371]  loss: (training)0.01195 (testing) 0.007, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00372]  loss: (training)0.01195 (testing) 0.007, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00373]  loss: (training)0.01194 (testing) 0.00699, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00374]  loss: (training)0.01194 (testing) 0.00699, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00375]  loss: (training)0.01194 (testing) 0.00699, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00376]  loss: (training)0.01193 (testing) 0.00698, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00377]  loss: (training)0.01193 (testing) 0.00698, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00378]  loss: (training)0.01193 (testing) 0.00698, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00379]  loss: (training)0.01192 (testing) 0.00697, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00380]  loss: (training)0.01192 (testing) 0.00697, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00381]  loss: (training)0.01192 (testing) 0.00697, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00382]  loss: (training)0.01191 (testing) 0.00696, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00383]  loss: (training)0.01191 (testing) 0.00696, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00384]  loss: (training)0.01191 (testing) 0.00696, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00385]  loss: (training)0.0119 (testing) 0.00695, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00386]  loss: (training)0.0119 (testing) 0.00695, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00387]  loss: (training)0.0119 (testing) 0.00694, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00388]  loss: (training)0.01189 (testing) 0.00694, accuracy: (training)94.20956 (testing) 86.32812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00389]  loss: (training)0.01189 (testing) 0.00694, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00390]  loss: (training)0.01189 (testing) 0.00693, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00391]  loss: (training)0.01188 (testing) 0.00693, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00392]  loss: (training)0.01188 (testing) 0.00693, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00393]  loss: (training)0.01188 (testing) 0.00692, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00394]  loss: (training)0.01187 (testing) 0.00692, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00395]  loss: (training)0.01187 (testing) 0.00692, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00396]  loss: (training)0.01187 (testing) 0.00691, accuracy: (training)94.20956 (testing) 86.32812\n",
      "[epoch 00397]  loss: (training)0.01186 (testing) 0.00691, accuracy: (training)94.30147 (testing) 86.32812\n",
      "[epoch 00398]  loss: (training)0.01186 (testing) 0.0069, accuracy: (training)94.30147 (testing) 86.32812\n",
      "[epoch 00399]  loss: (training)0.01185 (testing) 0.0069, accuracy: (training)94.30147 (testing) 86.32812\n",
      "[epoch 00400]  loss: (training)0.01185 (testing) 0.0069, accuracy: (training)94.39338 (testing) 86.32812\n",
      "[epoch 00401]  loss: (training)0.01185 (testing) 0.00689, accuracy: (training)94.39338 (testing) 86.32812\n",
      "[epoch 00402]  loss: (training)0.01184 (testing) 0.00689, accuracy: (training)94.39338 (testing) 86.32812\n",
      "[epoch 00403]  loss: (training)0.01184 (testing) 0.00689, accuracy: (training)94.39338 (testing) 86.32812\n",
      "[epoch 00404]  loss: (training)0.01184 (testing) 0.00688, accuracy: (training)94.48529 (testing) 86.71875\n",
      "[epoch 00405]  loss: (training)0.01183 (testing) 0.00688, accuracy: (training)94.48529 (testing) 86.71875\n",
      "[epoch 00406]  loss: (training)0.01183 (testing) 0.00688, accuracy: (training)94.48529 (testing) 86.71875\n",
      "[epoch 00407]  loss: (training)0.01183 (testing) 0.00687, accuracy: (training)94.48529 (testing) 87.10938\n",
      "[epoch 00408]  loss: (training)0.01182 (testing) 0.00687, accuracy: (training)94.48529 (testing) 87.10938\n",
      "[epoch 00409]  loss: (training)0.01182 (testing) 0.00687, accuracy: (training)94.57721 (testing) 87.10938\n",
      "[epoch 00410]  loss: (training)0.01182 (testing) 0.00686, accuracy: (training)94.66912 (testing) 87.10938\n",
      "[epoch 00411]  loss: (training)0.01181 (testing) 0.00686, accuracy: (training)94.66912 (testing) 87.10938\n",
      "[epoch 00412]  loss: (training)0.01181 (testing) 0.00686, accuracy: (training)94.66912 (testing) 87.10938\n",
      "[epoch 00413]  loss: (training)0.01181 (testing) 0.00685, accuracy: (training)94.66912 (testing) 87.10938\n",
      "[epoch 00414]  loss: (training)0.0118 (testing) 0.00685, accuracy: (training)94.76103 (testing) 87.10938\n",
      "[epoch 00415]  loss: (training)0.0118 (testing) 0.00685, accuracy: (training)94.76103 (testing) 87.10938\n",
      "[epoch 00416]  loss: (training)0.0118 (testing) 0.00685, accuracy: (training)94.94485 (testing) 87.10938\n",
      "[epoch 00417]  loss: (training)0.01179 (testing) 0.00684, accuracy: (training)94.94485 (testing) 87.10938\n",
      "[epoch 00418]  loss: (training)0.01179 (testing) 0.00684, accuracy: (training)95.03676 (testing) 87.10938\n",
      "[epoch 00419]  loss: (training)0.01179 (testing) 0.00684, accuracy: (training)95.03676 (testing) 87.5\n",
      "[epoch 00420]  loss: (training)0.01178 (testing) 0.00684, accuracy: (training)95.03676 (testing) 87.5\n",
      "[epoch 00421]  loss: (training)0.01178 (testing) 0.00683, accuracy: (training)95.03676 (testing) 87.89062\n",
      "[epoch 00422]  loss: (training)0.01178 (testing) 0.00683, accuracy: (training)95.03676 (testing) 87.89062\n",
      "[epoch 00423]  loss: (training)0.01177 (testing) 0.00683, accuracy: (training)95.03676 (testing) 87.89062\n",
      "[epoch 00424]  loss: (training)0.01177 (testing) 0.00683, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00425]  loss: (training)0.01177 (testing) 0.00682, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00426]  loss: (training)0.01176 (testing) 0.00682, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00427]  loss: (training)0.01176 (testing) 0.00682, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00428]  loss: (training)0.01176 (testing) 0.00682, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00429]  loss: (training)0.01175 (testing) 0.00682, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00430]  loss: (training)0.01175 (testing) 0.00681, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00431]  loss: (training)0.01175 (testing) 0.00681, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00432]  loss: (training)0.01174 (testing) 0.00681, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00433]  loss: (training)0.01174 (testing) 0.00681, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00434]  loss: (training)0.01174 (testing) 0.00681, accuracy: (training)95.03676 (testing) 88.28125\n",
      "[epoch 00435]  loss: (training)0.01173 (testing) 0.00681, accuracy: (training)95.12868 (testing) 87.89062\n",
      "[epoch 00436]  loss: (training)0.01173 (testing) 0.0068, accuracy: (training)95.12868 (testing) 87.89062\n",
      "[epoch 00437]  loss: (training)0.01173 (testing) 0.0068, accuracy: (training)95.12868 (testing) 87.89062\n",
      "[epoch 00438]  loss: (training)0.01172 (testing) 0.0068, accuracy: (training)95.12868 (testing) 87.89062\n",
      "[epoch 00439]  loss: (training)0.01172 (testing) 0.0068, accuracy: (training)95.12868 (testing) 87.89062\n",
      "[epoch 00440]  loss: (training)0.01172 (testing) 0.0068, accuracy: (training)95.22059 (testing) 87.89062\n",
      "[epoch 00441]  loss: (training)0.01171 (testing) 0.0068, accuracy: (training)95.22059 (testing) 87.89062\n",
      "[epoch 00442]  loss: (training)0.01171 (testing) 0.0068, accuracy: (training)95.3125 (testing) 87.89062\n",
      "[epoch 00443]  loss: (training)0.01171 (testing) 0.0068, accuracy: (training)95.3125 (testing) 87.89062\n",
      "[epoch 00444]  loss: (training)0.0117 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.89062\n",
      "[epoch 00445]  loss: (training)0.0117 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00446]  loss: (training)0.0117 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00447]  loss: (training)0.01169 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00448]  loss: (training)0.01169 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00449]  loss: (training)0.01169 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00450]  loss: (training)0.01168 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00451]  loss: (training)0.01168 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00452]  loss: (training)0.01168 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00453]  loss: (training)0.01167 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00454]  loss: (training)0.01167 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00455]  loss: (training)0.01167 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00456]  loss: (training)0.01166 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00457]  loss: (training)0.01166 (testing) 0.00679, accuracy: (training)95.40441 (testing) 87.5\n",
      "[epoch 00458]  loss: (training)0.01166 (testing) 0.00679, accuracy: (training)95.68015 (testing) 87.5\n",
      "[epoch 00459]  loss: (training)0.01166 (testing) 0.00678, accuracy: (training)95.68015 (testing) 87.5\n",
      "[epoch 00460]  loss: (training)0.01165 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00461]  loss: (training)0.01165 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00462]  loss: (training)0.01165 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00463]  loss: (training)0.01164 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00464]  loss: (training)0.01164 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00465]  loss: (training)0.01164 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00466]  loss: (training)0.01163 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00467]  loss: (training)0.01163 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00468]  loss: (training)0.01163 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00469]  loss: (training)0.01162 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00470]  loss: (training)0.01162 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00471]  loss: (training)0.01162 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00472]  loss: (training)0.01162 (testing) 0.00678, accuracy: (training)95.77206 (testing) 87.5\n",
      "[epoch 00473]  loss: (training)0.01161 (testing) 0.00678, accuracy: (training)95.86397 (testing) 87.5\n",
      "[epoch 00474]  loss: (training)0.01161 (testing) 0.00678, accuracy: (training)95.95588 (testing) 87.5\n",
      "[epoch 00475]  loss: (training)0.01161 (testing) 0.00678, accuracy: (training)95.95588 (testing) 87.5\n",
      "[epoch 00476]  loss: (training)0.0116 (testing) 0.00678, accuracy: (training)96.04779 (testing) 87.5\n",
      "[epoch 00477]  loss: (training)0.0116 (testing) 0.00678, accuracy: (training)96.13971 (testing) 87.5\n",
      "[epoch 00478]  loss: (training)0.0116 (testing) 0.00678, accuracy: (training)96.13971 (testing) 87.5\n",
      "[epoch 00479]  loss: (training)0.0116 (testing) 0.00678, accuracy: (training)96.13971 (testing) 87.5\n",
      "[epoch 00480]  loss: (training)0.01159 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00481]  loss: (training)0.01159 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00482]  loss: (training)0.01159 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00483]  loss: (training)0.01158 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00484]  loss: (training)0.01158 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00485]  loss: (training)0.01158 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00486]  loss: (training)0.01158 (testing) 0.00678, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00487]  loss: (training)0.01157 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00488]  loss: (training)0.01157 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00489]  loss: (training)0.01157 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.10938\n",
      "[epoch 00490]  loss: (training)0.01157 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00491]  loss: (training)0.01156 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00492]  loss: (training)0.01156 (testing) 0.00677, accuracy: (training)96.23162 (testing) 87.5\n",
      "[epoch 00493]  loss: (training)0.01156 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00494]  loss: (training)0.01155 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00495]  loss: (training)0.01155 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00496]  loss: (training)0.01155 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00497]  loss: (training)0.01155 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00498]  loss: (training)0.01154 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00499]  loss: (training)0.01154 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00500]  loss: (training)0.01154 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00501]  loss: (training)0.01153 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00502]  loss: (training)0.01153 (testing) 0.00677, accuracy: (training)96.41544 (testing) 87.5\n",
      "[epoch 00503]  loss: (training)0.01153 (testing) 0.00677, accuracy: (training)96.50735 (testing) 87.5\n",
      "[epoch 00504]  loss: (training)0.01153 (testing) 0.00677, accuracy: (training)96.50735 (testing) 87.89062\n",
      "[epoch 00505]  loss: (training)0.01152 (testing) 0.00677, accuracy: (training)96.50735 (testing) 87.89062\n",
      "[epoch 00506]  loss: (training)0.01152 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00507]  loss: (training)0.01152 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00508]  loss: (training)0.01151 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00509]  loss: (training)0.01151 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00510]  loss: (training)0.01151 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00511]  loss: (training)0.01151 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00512]  loss: (training)0.0115 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00513]  loss: (training)0.0115 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00514]  loss: (training)0.0115 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00515]  loss: (training)0.01149 (testing) 0.00676, accuracy: (training)96.59926 (testing) 87.89062\n",
      "[epoch 00516]  loss: (training)0.01149 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00517]  loss: (training)0.01149 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00518]  loss: (training)0.01149 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00519]  loss: (training)0.01148 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00520]  loss: (training)0.01148 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00521]  loss: (training)0.01148 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00522]  loss: (training)0.01147 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00523]  loss: (training)0.01147 (testing) 0.00676, accuracy: (training)96.59926 (testing) 88.28125\n",
      "[epoch 00524]  loss: (training)0.01147 (testing) 0.00676, accuracy: (training)96.69118 (testing) 88.28125\n",
      "[epoch 00525]  loss: (training)0.01147 (testing) 0.00676, accuracy: (training)96.69118 (testing) 88.28125\n",
      "[epoch 00526]  loss: (training)0.01146 (testing) 0.00676, accuracy: (training)96.69118 (testing) 88.28125\n",
      "[epoch 00527]  loss: (training)0.01146 (testing) 0.00676, accuracy: (training)96.69118 (testing) 88.28125\n",
      "[epoch 00528]  loss: (training)0.01146 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00529]  loss: (training)0.01145 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00530]  loss: (training)0.01145 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00531]  loss: (training)0.01145 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00532]  loss: (training)0.01144 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00533]  loss: (training)0.01144 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00534]  loss: (training)0.01144 (testing) 0.00676, accuracy: (training)96.78309 (testing) 88.28125\n",
      "[epoch 00535]  loss: (training)0.01144 (testing) 0.00676, accuracy: (training)96.875 (testing) 88.28125\n",
      "[epoch 00536]  loss: (training)0.01143 (testing) 0.00676, accuracy: (training)96.875 (testing) 88.28125\n",
      "[epoch 00537]  loss: (training)0.01143 (testing) 0.00676, accuracy: (training)96.875 (testing) 87.89062\n",
      "[epoch 00538]  loss: (training)0.01143 (testing) 0.00676, accuracy: (training)96.875 (testing) 87.89062\n",
      "[epoch 00539]  loss: (training)0.01142 (testing) 0.00676, accuracy: (training)96.875 (testing) 87.89062\n",
      "[epoch 00540]  loss: (training)0.01142 (testing) 0.00676, accuracy: (training)96.96691 (testing) 87.89062\n",
      "[epoch 00541]  loss: (training)0.01142 (testing) 0.00676, accuracy: (training)96.96691 (testing) 87.89062\n",
      "[epoch 00542]  loss: (training)0.01142 (testing) 0.00676, accuracy: (training)96.96691 (testing) 87.89062\n",
      "[epoch 00543]  loss: (training)0.01141 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.89062\n",
      "[epoch 00544]  loss: (training)0.01141 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.89062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00545]  loss: (training)0.01141 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.89062\n",
      "[epoch 00546]  loss: (training)0.01141 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.5\n",
      "[epoch 00547]  loss: (training)0.0114 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.5\n",
      "[epoch 00548]  loss: (training)0.0114 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.5\n",
      "[epoch 00549]  loss: (training)0.0114 (testing) 0.00676, accuracy: (training)97.05882 (testing) 87.5\n",
      "[epoch 00550]  loss: (training)0.0114 (testing) 0.00676, accuracy: (training)97.15074 (testing) 87.5\n",
      "[epoch 00551]  loss: (training)0.01139 (testing) 0.00676, accuracy: (training)97.15074 (testing) 87.5\n",
      "[epoch 00552]  loss: (training)0.01139 (testing) 0.00676, accuracy: (training)97.15074 (testing) 87.5\n",
      "[epoch 00553]  loss: (training)0.01139 (testing) 0.00675, accuracy: (training)97.15074 (testing) 87.5\n",
      "[epoch 00554]  loss: (training)0.01139 (testing) 0.00675, accuracy: (training)97.15074 (testing) 87.5\n",
      "[epoch 00555]  loss: (training)0.01138 (testing) 0.00675, accuracy: (training)97.24265 (testing) 87.5\n",
      "[epoch 00556]  loss: (training)0.01138 (testing) 0.00675, accuracy: (training)97.24265 (testing) 87.5\n",
      "[epoch 00557]  loss: (training)0.01138 (testing) 0.00675, accuracy: (training)97.24265 (testing) 87.5\n",
      "[epoch 00558]  loss: (training)0.01138 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00559]  loss: (training)0.01137 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00560]  loss: (training)0.01137 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00561]  loss: (training)0.01137 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00562]  loss: (training)0.01137 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00563]  loss: (training)0.01137 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.89062\n",
      "[epoch 00564]  loss: (training)0.01136 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.10938\n",
      "[epoch 00565]  loss: (training)0.01136 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.10938\n",
      "[epoch 00566]  loss: (training)0.01136 (testing) 0.00675, accuracy: (training)97.33456 (testing) 87.10938\n",
      "[epoch 00567]  loss: (training)0.01136 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00568]  loss: (training)0.01135 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00569]  loss: (training)0.01135 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00570]  loss: (training)0.01135 (testing) 0.00675, accuracy: (training)97.42647 (testing) 86.71875\n",
      "[epoch 00571]  loss: (training)0.01135 (testing) 0.00675, accuracy: (training)97.42647 (testing) 86.71875\n",
      "[epoch 00572]  loss: (training)0.01134 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00573]  loss: (training)0.01134 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00574]  loss: (training)0.01134 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.10938\n",
      "[epoch 00575]  loss: (training)0.01134 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.5\n",
      "[epoch 00576]  loss: (training)0.01134 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.5\n",
      "[epoch 00577]  loss: (training)0.01133 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00578]  loss: (training)0.01133 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00579]  loss: (training)0.01133 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00580]  loss: (training)0.01133 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00581]  loss: (training)0.01132 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00582]  loss: (training)0.01132 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00583]  loss: (training)0.01132 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00584]  loss: (training)0.01132 (testing) 0.00675, accuracy: (training)97.42647 (testing) 87.89062\n",
      "[epoch 00585]  loss: (training)0.01131 (testing) 0.00676, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00586]  loss: (training)0.01131 (testing) 0.00676, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00587]  loss: (training)0.01131 (testing) 0.00676, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00588]  loss: (training)0.01131 (testing) 0.00676, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00589]  loss: (training)0.01131 (testing) 0.00676, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00590]  loss: (training)0.0113 (testing) 0.00677, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00591]  loss: (training)0.0113 (testing) 0.00677, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00592]  loss: (training)0.0113 (testing) 0.00677, accuracy: (training)97.51838 (testing) 88.28125\n",
      "[epoch 00593]  loss: (training)0.0113 (testing) 0.00678, accuracy: (training)97.51838 (testing) 87.89062\n",
      "[epoch 00594]  loss: (training)0.0113 (testing) 0.00678, accuracy: (training)97.51838 (testing) 87.89062\n",
      "[epoch 00595]  loss: (training)0.0113 (testing) 0.00678, accuracy: (training)97.51838 (testing) 87.89062\n",
      "[epoch 00596]  loss: (training)0.01129 (testing) 0.00678, accuracy: (training)97.51838 (testing) 87.89062\n",
      "[epoch 00597]  loss: (training)0.01129 (testing) 0.00679, accuracy: (training)97.61029 (testing) 87.89062\n",
      "[epoch 00598]  loss: (training)0.01129 (testing) 0.00679, accuracy: (training)97.61029 (testing) 87.89062\n",
      "[epoch 00599]  loss: (training)0.01129 (testing) 0.0068, accuracy: (training)97.61029 (testing) 87.89062\n",
      "[epoch 00600]  loss: (training)0.01129 (testing) 0.0068, accuracy: (training)97.61029 (testing) 87.89062\n",
      "[epoch 00601]  loss: (training)0.01128 (testing) 0.0068, accuracy: (training)97.61029 (testing) 87.5\n",
      "[epoch 00602]  loss: (training)0.01128 (testing) 0.00681, accuracy: (training)97.61029 (testing) 87.5\n",
      "[epoch 00603]  loss: (training)0.01128 (testing) 0.00681, accuracy: (training)97.61029 (testing) 87.5\n",
      "[epoch 00604]  loss: (training)0.01128 (testing) 0.00682, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00605]  loss: (training)0.01128 (testing) 0.00682, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00606]  loss: (training)0.01128 (testing) 0.00682, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00607]  loss: (training)0.01128 (testing) 0.00683, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00608]  loss: (training)0.01127 (testing) 0.00683, accuracy: (training)97.61029 (testing) 87.5\n",
      "[epoch 00609]  loss: (training)0.01127 (testing) 0.00684, accuracy: (training)97.61029 (testing) 87.5\n",
      "[epoch 00610]  loss: (training)0.01127 (testing) 0.00684, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00611]  loss: (training)0.01127 (testing) 0.00684, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00612]  loss: (training)0.01127 (testing) 0.00685, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00613]  loss: (training)0.01127 (testing) 0.00685, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00614]  loss: (training)0.01126 (testing) 0.00686, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00615]  loss: (training)0.01126 (testing) 0.00686, accuracy: (training)97.61029 (testing) 87.10938\n",
      "[epoch 00616]  loss: (training)0.01126 (testing) 0.00686, accuracy: (training)97.70221 (testing) 87.10938\n",
      "[epoch 00617]  loss: (training)0.01126 (testing) 0.00687, accuracy: (training)97.70221 (testing) 87.10938\n",
      "[epoch 00618]  loss: (training)0.01126 (testing) 0.00687, accuracy: (training)97.70221 (testing) 87.10938\n",
      "[epoch 00619]  loss: (training)0.01126 (testing) 0.00688, accuracy: (training)97.70221 (testing) 86.71875\n",
      "[epoch 00620]  loss: (training)0.01125 (testing) 0.00688, accuracy: (training)97.70221 (testing) 86.71875\n",
      "[epoch 00621]  loss: (training)0.01125 (testing) 0.00688, accuracy: (training)97.70221 (testing) 86.71875\n",
      "[epoch 00622]  loss: (training)0.01125 (testing) 0.00689, accuracy: (training)97.70221 (testing) 86.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00623]  loss: (training)0.01125 (testing) 0.00689, accuracy: (training)97.70221 (testing) 86.71875\n",
      "[epoch 00624]  loss: (training)0.01125 (testing) 0.0069, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00625]  loss: (training)0.01125 (testing) 0.0069, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00626]  loss: (training)0.01125 (testing) 0.0069, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00627]  loss: (training)0.01124 (testing) 0.00691, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00628]  loss: (training)0.01124 (testing) 0.00691, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00629]  loss: (training)0.01124 (testing) 0.00691, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00630]  loss: (training)0.01124 (testing) 0.00692, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00631]  loss: (training)0.01124 (testing) 0.00692, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00632]  loss: (training)0.01124 (testing) 0.00693, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00633]  loss: (training)0.01124 (testing) 0.00693, accuracy: (training)97.70221 (testing) 86.32812\n",
      "[epoch 00634]  loss: (training)0.01124 (testing) 0.00693, accuracy: (training)97.79412 (testing) 86.32812\n",
      "[epoch 00635]  loss: (training)0.01123 (testing) 0.00694, accuracy: (training)97.79412 (testing) 86.32812\n",
      "[epoch 00636]  loss: (training)0.01123 (testing) 0.00694, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00637]  loss: (training)0.01123 (testing) 0.00694, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00638]  loss: (training)0.01123 (testing) 0.00695, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00639]  loss: (training)0.01123 (testing) 0.00695, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00640]  loss: (training)0.01123 (testing) 0.00696, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00641]  loss: (training)0.01123 (testing) 0.00696, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00642]  loss: (training)0.01122 (testing) 0.00696, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00643]  loss: (training)0.01122 (testing) 0.00697, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00644]  loss: (training)0.01122 (testing) 0.00697, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00645]  loss: (training)0.01122 (testing) 0.00697, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00646]  loss: (training)0.01122 (testing) 0.00698, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00647]  loss: (training)0.01122 (testing) 0.00698, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00648]  loss: (training)0.01122 (testing) 0.00699, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00649]  loss: (training)0.01122 (testing) 0.00699, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00650]  loss: (training)0.01122 (testing) 0.00699, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00651]  loss: (training)0.01121 (testing) 0.007, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00652]  loss: (training)0.01121 (testing) 0.007, accuracy: (training)97.79412 (testing) 85.9375\n",
      "[epoch 00653]  loss: (training)0.01121 (testing) 0.007, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00654]  loss: (training)0.01121 (testing) 0.00701, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00655]  loss: (training)0.01121 (testing) 0.00701, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00656]  loss: (training)0.01121 (testing) 0.00702, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00657]  loss: (training)0.01121 (testing) 0.00702, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00658]  loss: (training)0.01121 (testing) 0.00702, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00659]  loss: (training)0.01121 (testing) 0.00703, accuracy: (training)97.97794 (testing) 85.9375\n",
      "[epoch 00660]  loss: (training)0.0112 (testing) 0.00703, accuracy: (training)97.97794 (testing) 85.9375\n",
      "[epoch 00661]  loss: (training)0.0112 (testing) 0.00704, accuracy: (training)97.97794 (testing) 85.9375\n",
      "[epoch 00662]  loss: (training)0.0112 (testing) 0.00704, accuracy: (training)97.97794 (testing) 85.9375\n",
      "[epoch 00663]  loss: (training)0.0112 (testing) 0.00705, accuracy: (training)97.97794 (testing) 85.54688\n",
      "[epoch 00664]  loss: (training)0.0112 (testing) 0.00705, accuracy: (training)97.97794 (testing) 85.54688\n",
      "[epoch 00665]  loss: (training)0.0112 (testing) 0.00705, accuracy: (training)97.97794 (testing) 85.54688\n",
      "[epoch 00666]  loss: (training)0.0112 (testing) 0.00706, accuracy: (training)97.97794 (testing) 85.54688\n",
      "[epoch 00667]  loss: (training)0.0112 (testing) 0.00706, accuracy: (training)97.97794 (testing) 85.54688\n",
      "[epoch 00668]  loss: (training)0.0112 (testing) 0.00706, accuracy: (training)97.97794 (testing) 85.54688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2e27ee146615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mresult_train\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresult_test\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mresult_loss_train_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_train_mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-657cb256276f>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbCuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 2000\n",
    "loss_train_mean = list()\n",
    "loss_train_std = list()\n",
    "acc_train_mean = list()\n",
    "acc_train_std = list()\n",
    "loss_test = list()\n",
    "accuracy_test = list()\n",
    "\n",
    "for e in range(epoch):\n",
    "        \n",
    "    result_train    = train()\n",
    "    result_test     = test()\n",
    "    \n",
    "    result_loss_train_mean = result_train['loss_train_mean']\n",
    "    result_loss_train_std =  result_train['loss_train_std']\n",
    "    result_acc_train_mean =  result_train['acc_train_mean']\n",
    "    result_acc_train_std =  result_train['acc_train_std']\n",
    "    result_loss_test =       result_test['loss_test']\n",
    "    result_accuracy_test =   result_test['accuracy_test']\n",
    "\n",
    "    loss_train_mean.append( result_loss_train_mean)\n",
    "    loss_train_std.append(  result_loss_train_std)\n",
    "    acc_train_mean.append(  result_acc_train_mean)\n",
    "    acc_train_std.append(  result_acc_train_std)\n",
    "    loss_test.append(       result_loss_test)\n",
    "    accuracy_test.append(   result_accuracy_test)\n",
    "    \n",
    "    epoch_str = '[epoch '+ '{:05d}'.format(e)+ ']  '\n",
    "    epoch_str += 'loss: '+ '(training)' + str(round(result_loss_train_mean, 5))+ ' (testing) ' + str(round(result_loss_test, 5)) + ', '\n",
    "    epoch_str += 'accuracy: '+ '(training)' + str(round(result_acc_train_mean, 5))+ ' (testing) ' + str(round(result_accuracy_test, 5))\n",
    "    print(epoch_str)\n",
    "    \n",
    "    #adaptive learning rate\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = g['lr'] * 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history_graph(loss_history, loss_train_std, acc_history, acc_train_std, vloss_history, vacc_history):\n",
    "    #plt.plot(acc_history, color='#ff0000', label='Train Accuracy')\n",
    "    plt.errorbar(list(range(len(acc_history), acc_history, yerr=acc_train_std, ecolor = '#ffcccc', color='#ff0000', label='Train Loss')\n",
    "    plt.plot(vacc_history, color='#0000ff', label='Validation Accuracy')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(loss_history, color='#ff0000', label='Train Loss')\n",
    "    plt.errorbar(list(range(len(loss_history), loss_history, yerr=loss_train_std, ecolor = '#ffcccc', color='#ff0000', label='Train Loss')\n",
    "    plt.plot(vloss_history, color='#0000ff', label='Validation Loss')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    nx, ny = 3, 3\n",
    "    data = (('dataset', 'loss', 'accuracy'), ('train', str(np.round(loss_history[-1],4)), str(np.round(acc_history[-1],4))), ('validation', str(np.round(vloss_history[-1],4)), str(np.round(vacc_history[-1],4))))\n",
    "    pl.figure()\n",
    "    tb = pl.table(cellText=data, loc=(0,0), cellLoc='center')\n",
    "\n",
    "    tc = tb.properties()['child_artists']\n",
    "    for cell in tc: \n",
    "        cell.set_height(1/ny)\n",
    "        cell.set_width(1/nx)\n",
    "\n",
    "    ax = pl.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_history_graph(loss_train_mean, loss_train_std, acc_train_mean, acc_train_std, loss_test, accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
