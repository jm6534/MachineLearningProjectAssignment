{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOro3YNhQZs7"
   },
   "source": [
    "### Binary classification based on 3 layers neural network\n",
    "#### author: Kim Jeong Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIFz7H8yQZs9"
   },
   "source": [
    "##### load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3u1rh-hqQZs9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime \n",
    "import csv\n",
    "import configparser\n",
    "import argparse\n",
    "import platform\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lf7hA6zQZtA"
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2, p=0.0):\n",
    "\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.ms = list()\n",
    "        \n",
    "        self.number_class   = num_classes\n",
    "\n",
    "        _size_image     = 100* 100\n",
    "        _num1           = 512\n",
    "        _num2           = 128\n",
    "        _num3           = 64\n",
    "        \n",
    "        self.fc1        = nn.Linear(_size_image, _num1, bias=True)\n",
    "        self.fc2        = nn.Linear(_num1, _num2, bias=True)\n",
    "        self.fc3        = nn.Linear(_num2, _num3, bias=True)\n",
    "        self.fc4        = nn.Linear(_num3, num_classes, bias=True)\n",
    "        self.ms.append(self.fc1)\n",
    "        self.ms.append(self.fc2)\n",
    "        self.ms.append(self.fc3)\n",
    "        self.ms.append(self.fc4)\n",
    "\n",
    "        self.fc_layer1  = nn.Sequential(self.fc1, nn.ReLU())\n",
    "        self.fc_layer2  = nn.Sequential(self.fc2, nn.ReLU())\n",
    "        self.fc_layer3  = nn.Sequential(self.fc3, nn.ReLU())\n",
    "        self.fc_layer4  = nn.Sequential(self.fc4, nn.Sigmoid())\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2, self.fc_layer3, self.fc_layer4)\n",
    "        \n",
    "        self._initialize_weight()\n",
    "        \n",
    "        self.p = p\n",
    "        self.dropout = nn.Dropout(p=self.p)\n",
    "        \n",
    "    def _initialize_weight(self):\n",
    "\n",
    "        for m in self.ms:\n",
    "            \n",
    "            n = m.in_features\n",
    "            m.weight.data.uniform_(- 1.0 / math.sqrt(n), 1.0 / math.sqrt(n))\n",
    "\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        #x = self.dropout(F.relu(self.fc2(x)))\n",
    "        #x = self.dropout(F.relu(self.fc3(x)))\n",
    "        #x = F.softmax(self.fc4(x),dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhDyBWYVQZtC"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "drive_prefix = 'drive/My Drive/'\n",
    "train_data_path = drive_prefix + 'horse-or-human/train'\n",
    "trains_set = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "loader_train = torch.utils.data.DataLoader(trains_set, batch_size=64, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "test_data_path = drive_prefix + 'horse-or-human/validation'\n",
    "test_set = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "loader_test = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=1)  \n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLLNfscxQZtE"
   },
   "source": [
    "##### load neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IopWIJWQZtE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Linear(num_classes=num_classes, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oA41bGU_QZtG"
   },
   "source": [
    "##### Set the flag for using cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J40z1SyRQZtH"
   },
   "outputs": [],
   "source": [
    "bCuda = True\n",
    "if bCuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCRBLDUhQZtI"
   },
   "source": [
    "##### optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uN1EBjMBQZtJ"
   },
   "outputs": [],
   "source": [
    "LR = 0.02\n",
    "optimizer   = optim.SGD(model.parameters(), lr=LR, weight_decay=1e-3 * 5)\n",
    "objective   = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xc-HJgCQZtL"
   },
   "source": [
    "##### function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjXKsB1aQZtL"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # print('train the model at given epoch')\n",
    "    loss_train          = []\n",
    "    acc_train           = []\n",
    "    model.train()\n",
    "    for idx_batch, (data, target) in enumerate(loader_train):\n",
    "        if bCuda:\n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "            \n",
    "        data, target    = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_batch    = loss.item() / len(data)\n",
    "        loss_train.append(loss_train_batch)\n",
    "        \n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        acc_train_batch   = 100. * float(correct) / len(data)\n",
    "        acc_train.append(acc_train_batch)\n",
    "        \n",
    "    loss_train_mean     = np.mean(loss_train)\n",
    "    loss_train_std      = np.std(loss_train)\n",
    "    acc_train_mean      = np.mean(acc_train)\n",
    "    acc_train_std       = np.std(acc_train)\n",
    "\n",
    "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std, 'acc_train_mean': acc_train_mean, 'acc_train_std': acc_train_std}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAGLuK66QZtN"
   },
   "source": [
    "##### function for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vs6mQq0xQZtO"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # print('test the model at given epoch')\n",
    "\n",
    "    accuracy_test   = []\n",
    "    loss_test       = 0\n",
    "    correct         = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx_batch, (data, target) in enumerate(loader_test):\n",
    "\n",
    "        if bCuda:\n",
    "        \n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss_test   += loss.item()\n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss_test       = loss_test / len(loader_test.dataset)\n",
    "    accuracy_test   = 100. * float(correct) / len(loader_test.dataset)\n",
    "\n",
    "    return {'loss_test': loss_test, 'accuracy_test': accuracy_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uo7bMVqAQZtP"
   },
   "source": [
    "##### iteration for the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QOV9Rt9JQZtQ",
    "outputId": "bd49ff3a-8122-4844-ce7b-f450382692fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00000]  loss: (training)0.01333 (testing) 0.01083, accuracy: (training)56.43382 (testing) 50.0\n",
      "[epoch 00001]  loss: (training)0.01331 (testing) 0.01081, accuracy: (training)54.77941 (testing) 50.0\n",
      "[epoch 00002]  loss: (training)0.01328 (testing) 0.01081, accuracy: (training)52.20588 (testing) 50.0\n",
      "[epoch 00003]  loss: (training)0.01326 (testing) 0.01081, accuracy: (training)51.65441 (testing) 50.0\n",
      "[epoch 00004]  loss: (training)0.01324 (testing) 0.0108, accuracy: (training)51.37868 (testing) 50.0\n",
      "[epoch 00005]  loss: (training)0.01322 (testing) 0.0108, accuracy: (training)51.93015 (testing) 50.0\n",
      "[epoch 00006]  loss: (training)0.01319 (testing) 0.0108, accuracy: (training)54.96324 (testing) 50.0\n",
      "[epoch 00007]  loss: (training)0.01317 (testing) 0.0108, accuracy: (training)55.33088 (testing) 50.0\n",
      "[epoch 00008]  loss: (training)0.01315 (testing) 0.0108, accuracy: (training)55.88235 (testing) 50.0\n",
      "[epoch 00009]  loss: (training)0.01313 (testing) 0.01081, accuracy: (training)55.33088 (testing) 50.0\n",
      "[epoch 00010]  loss: (training)0.01311 (testing) 0.01081, accuracy: (training)55.23897 (testing) 50.0\n",
      "[epoch 00011]  loss: (training)0.01308 (testing) 0.01081, accuracy: (training)54.96324 (testing) 50.0\n",
      "[epoch 00012]  loss: (training)0.01306 (testing) 0.01082, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00013]  loss: (training)0.01303 (testing) 0.01083, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00014]  loss: (training)0.01301 (testing) 0.01084, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00015]  loss: (training)0.01298 (testing) 0.01085, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00016]  loss: (training)0.01296 (testing) 0.01086, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00017]  loss: (training)0.01293 (testing) 0.01088, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00018]  loss: (training)0.0129 (testing) 0.01089, accuracy: (training)54.6875 (testing) 50.0\n",
      "[epoch 00019]  loss: (training)0.01288 (testing) 0.01091, accuracy: (training)54.87132 (testing) 50.0\n",
      "[epoch 00020]  loss: (training)0.01285 (testing) 0.01093, accuracy: (training)54.96324 (testing) 50.0\n",
      "[epoch 00021]  loss: (training)0.01282 (testing) 0.01094, accuracy: (training)55.05515 (testing) 50.0\n",
      "[epoch 00022]  loss: (training)0.0128 (testing) 0.01096, accuracy: (training)55.42279 (testing) 50.0\n",
      "[epoch 00023]  loss: (training)0.01277 (testing) 0.01098, accuracy: (training)55.60662 (testing) 50.0\n",
      "[epoch 00024]  loss: (training)0.01275 (testing) 0.01099, accuracy: (training)55.88235 (testing) 50.0\n",
      "[epoch 00025]  loss: (training)0.01272 (testing) 0.01101, accuracy: (training)56.43382 (testing) 50.0\n",
      "[epoch 00026]  loss: (training)0.01269 (testing) 0.01102, accuracy: (training)56.25 (testing) 50.0\n",
      "[epoch 00027]  loss: (training)0.01266 (testing) 0.01104, accuracy: (training)55.88235 (testing) 50.0\n",
      "[epoch 00028]  loss: (training)0.01263 (testing) 0.01105, accuracy: (training)56.25 (testing) 50.0\n",
      "[epoch 00029]  loss: (training)0.0126 (testing) 0.01107, accuracy: (training)55.88235 (testing) 50.0\n",
      "[epoch 00030]  loss: (training)0.01256 (testing) 0.01108, accuracy: (training)55.51471 (testing) 50.0\n",
      "[epoch 00031]  loss: (training)0.01253 (testing) 0.0111, accuracy: (training)55.60662 (testing) 50.0\n",
      "[epoch 00032]  loss: (training)0.01249 (testing) 0.01112, accuracy: (training)55.42279 (testing) 50.0\n",
      "[epoch 00033]  loss: (training)0.01245 (testing) 0.01113, accuracy: (training)56.70956 (testing) 50.0\n",
      "[epoch 00034]  loss: (training)0.0124 (testing) 0.01115, accuracy: (training)57.16912 (testing) 50.0\n",
      "[epoch 00035]  loss: (training)0.01235 (testing) 0.01117, accuracy: (training)57.62868 (testing) 50.0\n",
      "[epoch 00036]  loss: (training)0.01229 (testing) 0.0112, accuracy: (training)57.99632 (testing) 50.0\n",
      "[epoch 00037]  loss: (training)0.01223 (testing) 0.01123, accuracy: (training)57.53676 (testing) 50.0\n",
      "[epoch 00038]  loss: (training)0.01217 (testing) 0.01126, accuracy: (training)58.36397 (testing) 50.0\n",
      "[epoch 00039]  loss: (training)0.0121 (testing) 0.01129, accuracy: (training)58.54779 (testing) 50.0\n",
      "[epoch 00040]  loss: (training)0.01203 (testing) 0.01133, accuracy: (training)59.55882 (testing) 50.0\n",
      "[epoch 00041]  loss: (training)0.01196 (testing) 0.01137, accuracy: (training)60.29412 (testing) 50.0\n",
      "[epoch 00042]  loss: (training)0.01189 (testing) 0.0114, accuracy: (training)60.9375 (testing) 50.0\n",
      "[epoch 00043]  loss: (training)0.01182 (testing) 0.01143, accuracy: (training)60.9375 (testing) 50.0\n",
      "[epoch 00044]  loss: (training)0.01175 (testing) 0.01146, accuracy: (training)61.12132 (testing) 50.0\n",
      "[epoch 00045]  loss: (training)0.01168 (testing) 0.01148, accuracy: (training)61.94853 (testing) 50.0\n",
      "[epoch 00046]  loss: (training)0.0116 (testing) 0.0115, accuracy: (training)62.40809 (testing) 50.0\n",
      "[epoch 00047]  loss: (training)0.01154 (testing) 0.01151, accuracy: (training)63.23529 (testing) 50.0\n",
      "[epoch 00048]  loss: (training)0.01147 (testing) 0.01152, accuracy: (training)63.51103 (testing) 50.0\n",
      "[epoch 00049]  loss: (training)0.01139 (testing) 0.01152, accuracy: (training)63.87868 (testing) 50.0\n",
      "[epoch 00050]  loss: (training)0.01132 (testing) 0.01152, accuracy: (training)63.78676 (testing) 50.0\n",
      "[epoch 00051]  loss: (training)0.01125 (testing) 0.01152, accuracy: (training)64.15441 (testing) 50.0\n",
      "[epoch 00052]  loss: (training)0.01117 (testing) 0.01151, accuracy: (training)64.98162 (testing) 50.0\n",
      "[epoch 00053]  loss: (training)0.0111 (testing) 0.0115, accuracy: (training)65.44118 (testing) 50.0\n",
      "[epoch 00054]  loss: (training)0.01102 (testing) 0.01148, accuracy: (training)65.90074 (testing) 50.0\n",
      "[epoch 00055]  loss: (training)0.01095 (testing) 0.01146, accuracy: (training)66.54412 (testing) 50.0\n",
      "[epoch 00056]  loss: (training)0.01087 (testing) 0.01143, accuracy: (training)67.37132 (testing) 50.0\n",
      "[epoch 00057]  loss: (training)0.0108 (testing) 0.01141, accuracy: (training)67.92279 (testing) 50.78125\n",
      "[epoch 00058]  loss: (training)0.01073 (testing) 0.01138, accuracy: (training)68.56618 (testing) 51.17188\n",
      "[epoch 00059]  loss: (training)0.01065 (testing) 0.01136, accuracy: (training)69.02574 (testing) 51.17188\n",
      "[epoch 00060]  loss: (training)0.01058 (testing) 0.01133, accuracy: (training)69.57721 (testing) 51.17188\n",
      "[epoch 00061]  loss: (training)0.01052 (testing) 0.0113, accuracy: (training)70.03676 (testing) 51.17188\n",
      "[epoch 00062]  loss: (training)0.01045 (testing) 0.01126, accuracy: (training)70.22059 (testing) 51.17188\n",
      "[epoch 00063]  loss: (training)0.01039 (testing) 0.01122, accuracy: (training)70.86397 (testing) 51.17188\n",
      "[epoch 00064]  loss: (training)0.01032 (testing) 0.01119, accuracy: (training)71.23162 (testing) 51.5625\n",
      "[epoch 00065]  loss: (training)0.01026 (testing) 0.01115, accuracy: (training)71.41544 (testing) 52.73438\n",
      "[epoch 00066]  loss: (training)0.0102 (testing) 0.01111, accuracy: (training)72.05882 (testing) 53.125\n",
      "[epoch 00067]  loss: (training)0.01015 (testing) 0.01107, accuracy: (training)72.42647 (testing) 53.125\n",
      "[epoch 00068]  loss: (training)0.01009 (testing) 0.01103, accuracy: (training)72.88603 (testing) 54.6875\n",
      "[epoch 00069]  loss: (training)0.01004 (testing) 0.01099, accuracy: (training)73.34559 (testing) 54.6875\n",
      "[epoch 00070]  loss: (training)0.00999 (testing) 0.01094, accuracy: (training)73.62132 (testing) 55.07812\n",
      "[epoch 00071]  loss: (training)0.00994 (testing) 0.01089, accuracy: (training)74.08088 (testing) 55.46875\n",
      "[epoch 00072]  loss: (training)0.00989 (testing) 0.01085, accuracy: (training)74.26471 (testing) 55.85938\n",
      "[epoch 00073]  loss: (training)0.00984 (testing) 0.0108, accuracy: (training)74.54044 (testing) 55.85938\n",
      "[epoch 00074]  loss: (training)0.00979 (testing) 0.01075, accuracy: (training)74.90809 (testing) 55.85938\n",
      "[epoch 00075]  loss: (training)0.00974 (testing) 0.0107, accuracy: (training)75.18382 (testing) 56.64062\n",
      "[epoch 00076]  loss: (training)0.00969 (testing) 0.01064, accuracy: (training)75.55147 (testing) 57.03125\n",
      "[epoch 00077]  loss: (training)0.00965 (testing) 0.01059, accuracy: (training)76.01103 (testing) 57.03125\n",
      "[epoch 00078]  loss: (training)0.0096 (testing) 0.01053, accuracy: (training)76.37868 (testing) 57.42188\n",
      "[epoch 00079]  loss: (training)0.00955 (testing) 0.01048, accuracy: (training)76.37868 (testing) 57.8125\n",
      "[epoch 00080]  loss: (training)0.0095 (testing) 0.01042, accuracy: (training)76.65441 (testing) 58.20312\n",
      "[epoch 00081]  loss: (training)0.00946 (testing) 0.01036, accuracy: (training)77.48162 (testing) 58.59375\n",
      "[epoch 00082]  loss: (training)0.00941 (testing) 0.0103, accuracy: (training)77.48162 (testing) 59.375\n",
      "[epoch 00083]  loss: (training)0.00937 (testing) 0.01024, accuracy: (training)77.75735 (testing) 60.15625\n",
      "[epoch 00084]  loss: (training)0.00933 (testing) 0.01018, accuracy: (training)78.125 (testing) 60.54688\n",
      "[epoch 00085]  loss: (training)0.00929 (testing) 0.01012, accuracy: (training)78.40074 (testing) 60.9375\n",
      "[epoch 00086]  loss: (training)0.00925 (testing) 0.01007, accuracy: (training)78.58456 (testing) 61.32812\n",
      "[epoch 00087]  loss: (training)0.00921 (testing) 0.01001, accuracy: (training)78.76838 (testing) 62.89062\n",
      "[epoch 00088]  loss: (training)0.00917 (testing) 0.00996, accuracy: (training)79.41176 (testing) 62.89062\n",
      "[epoch 00089]  loss: (training)0.00913 (testing) 0.00991, accuracy: (training)79.87132 (testing) 63.67188\n",
      "[epoch 00090]  loss: (training)0.0091 (testing) 0.00986, accuracy: (training)80.14706 (testing) 64.45312\n",
      "[epoch 00091]  loss: (training)0.00906 (testing) 0.00981, accuracy: (training)80.23897 (testing) 65.23438\n",
      "[epoch 00092]  loss: (training)0.00903 (testing) 0.00976, accuracy: (training)80.69853 (testing) 65.23438\n",
      "[epoch 00093]  loss: (training)0.00899 (testing) 0.00971, accuracy: (training)81.06618 (testing) 65.23438\n",
      "[epoch 00094]  loss: (training)0.00896 (testing) 0.00967, accuracy: (training)81.25 (testing) 65.625\n",
      "[epoch 00095]  loss: (training)0.00893 (testing) 0.00962, accuracy: (training)81.43382 (testing) 67.1875\n",
      "[epoch 00096]  loss: (training)0.0089 (testing) 0.00958, accuracy: (training)81.43382 (testing) 67.96875\n",
      "[epoch 00097]  loss: (training)0.00887 (testing) 0.00954, accuracy: (training)81.80147 (testing) 67.96875\n",
      "[epoch 00098]  loss: (training)0.00884 (testing) 0.00949, accuracy: (training)81.89338 (testing) 67.96875\n",
      "[epoch 00099]  loss: (training)0.00881 (testing) 0.00945, accuracy: (training)82.26103 (testing) 67.96875\n",
      "[epoch 00100]  loss: (training)0.00878 (testing) 0.00941, accuracy: (training)82.44485 (testing) 68.35938\n",
      "[epoch 00101]  loss: (training)0.00875 (testing) 0.00938, accuracy: (training)82.62868 (testing) 68.75\n",
      "[epoch 00102]  loss: (training)0.00873 (testing) 0.00934, accuracy: (training)82.99632 (testing) 68.75\n",
      "[epoch 00103]  loss: (training)0.0087 (testing) 0.00931, accuracy: (training)83.27206 (testing) 68.75\n",
      "[epoch 00104]  loss: (training)0.00867 (testing) 0.00928, accuracy: (training)83.36397 (testing) 69.14062\n",
      "[epoch 00105]  loss: (training)0.00864 (testing) 0.00924, accuracy: (training)83.82353 (testing) 69.14062\n",
      "[epoch 00106]  loss: (training)0.00862 (testing) 0.00921, accuracy: (training)83.82353 (testing) 69.14062\n",
      "[epoch 00107]  loss: (training)0.00859 (testing) 0.00918, accuracy: (training)84.00735 (testing) 69.14062\n",
      "[epoch 00108]  loss: (training)0.00856 (testing) 0.00916, accuracy: (training)84.09926 (testing) 69.92188\n",
      "[epoch 00109]  loss: (training)0.00854 (testing) 0.00913, accuracy: (training)84.09926 (testing) 70.70312\n",
      "[epoch 00110]  loss: (training)0.00851 (testing) 0.0091, accuracy: (training)84.09926 (testing) 71.09375\n",
      "[epoch 00111]  loss: (training)0.00848 (testing) 0.00908, accuracy: (training)84.19118 (testing) 71.48438\n",
      "[epoch 00112]  loss: (training)0.00846 (testing) 0.00906, accuracy: (training)84.55882 (testing) 71.48438\n",
      "[epoch 00113]  loss: (training)0.00843 (testing) 0.00904, accuracy: (training)84.74265 (testing) 71.48438\n",
      "[epoch 00114]  loss: (training)0.00841 (testing) 0.00902, accuracy: (training)84.92647 (testing) 71.875\n",
      "[epoch 00115]  loss: (training)0.00838 (testing) 0.009, accuracy: (training)85.01838 (testing) 72.26562\n",
      "[epoch 00116]  loss: (training)0.00835 (testing) 0.00898, accuracy: (training)85.29412 (testing) 72.26562\n",
      "[epoch 00117]  loss: (training)0.00833 (testing) 0.00896, accuracy: (training)85.47794 (testing) 72.26562\n",
      "[epoch 00118]  loss: (training)0.00831 (testing) 0.00894, accuracy: (training)85.66176 (testing) 72.26562\n",
      "[epoch 00119]  loss: (training)0.00828 (testing) 0.00893, accuracy: (training)86.12132 (testing) 72.26562\n",
      "[epoch 00120]  loss: (training)0.00826 (testing) 0.00891, accuracy: (training)86.30515 (testing) 72.26562\n",
      "[epoch 00121]  loss: (training)0.00824 (testing) 0.0089, accuracy: (training)86.30515 (testing) 72.65625\n",
      "[epoch 00122]  loss: (training)0.00821 (testing) 0.00889, accuracy: (training)86.30515 (testing) 72.65625\n",
      "[epoch 00123]  loss: (training)0.00819 (testing) 0.00888, accuracy: (training)86.39706 (testing) 72.65625\n",
      "[epoch 00124]  loss: (training)0.00817 (testing) 0.00887, accuracy: (training)86.48897 (testing) 72.65625\n",
      "[epoch 00125]  loss: (training)0.00815 (testing) 0.00886, accuracy: (training)86.67279 (testing) 72.65625\n",
      "[epoch 00126]  loss: (training)0.00812 (testing) 0.00885, accuracy: (training)87.13235 (testing) 72.65625\n",
      "[epoch 00127]  loss: (training)0.0081 (testing) 0.00884, accuracy: (training)87.31618 (testing) 73.04688\n",
      "[epoch 00128]  loss: (training)0.00808 (testing) 0.00884, accuracy: (training)87.5 (testing) 73.04688\n",
      "[epoch 00129]  loss: (training)0.00806 (testing) 0.00884, accuracy: (training)87.95956 (testing) 73.04688\n",
      "[epoch 00130]  loss: (training)0.00804 (testing) 0.00883, accuracy: (training)88.14338 (testing) 73.04688\n",
      "[epoch 00131]  loss: (training)0.00802 (testing) 0.00883, accuracy: (training)88.41912 (testing) 73.04688\n",
      "[epoch 00132]  loss: (training)0.008 (testing) 0.00883, accuracy: (training)88.51103 (testing) 73.04688\n",
      "[epoch 00133]  loss: (training)0.00798 (testing) 0.00883, accuracy: (training)88.69485 (testing) 73.04688\n",
      "[epoch 00134]  loss: (training)0.00796 (testing) 0.00883, accuracy: (training)88.78676 (testing) 73.04688\n",
      "[epoch 00135]  loss: (training)0.00793 (testing) 0.00884, accuracy: (training)88.97059 (testing) 73.04688\n",
      "[epoch 00136]  loss: (training)0.00792 (testing) 0.00884, accuracy: (training)89.15441 (testing) 73.04688\n",
      "[epoch 00137]  loss: (training)0.0079 (testing) 0.00884, accuracy: (training)89.33824 (testing) 73.04688\n",
      "[epoch 00138]  loss: (training)0.00788 (testing) 0.00884, accuracy: (training)89.61397 (testing) 72.65625\n",
      "[epoch 00139]  loss: (training)0.00786 (testing) 0.00885, accuracy: (training)89.61397 (testing) 73.04688\n",
      "[epoch 00140]  loss: (training)0.00784 (testing) 0.00885, accuracy: (training)89.61397 (testing) 73.04688\n",
      "[epoch 00141]  loss: (training)0.00783 (testing) 0.00886, accuracy: (training)89.88971 (testing) 73.04688\n",
      "[epoch 00142]  loss: (training)0.00781 (testing) 0.00886, accuracy: (training)89.88971 (testing) 73.04688\n",
      "[epoch 00143]  loss: (training)0.0078 (testing) 0.00887, accuracy: (training)89.88971 (testing) 73.04688\n",
      "[epoch 00144]  loss: (training)0.00779 (testing) 0.00886, accuracy: (training)90.07353 (testing) 73.04688\n",
      "[epoch 00145]  loss: (training)0.00777 (testing) 0.00887, accuracy: (training)90.16544 (testing) 73.04688\n",
      "[epoch 00146]  loss: (training)0.00776 (testing) 0.00888, accuracy: (training)90.16544 (testing) 73.04688\n",
      "[epoch 00147]  loss: (training)0.00775 (testing) 0.00889, accuracy: (training)90.34926 (testing) 73.04688\n",
      "[epoch 00148]  loss: (training)0.00775 (testing) 0.00889, accuracy: (training)90.34926 (testing) 73.04688\n",
      "[epoch 00149]  loss: (training)0.00774 (testing) 0.0089, accuracy: (training)90.34926 (testing) 73.04688\n",
      "[epoch 00150]  loss: (training)0.00773 (testing) 0.00891, accuracy: (training)90.53309 (testing) 73.04688\n",
      "[epoch 00151]  loss: (training)0.00772 (testing) 0.00891, accuracy: (training)90.625 (testing) 73.04688\n",
      "[epoch 00152]  loss: (training)0.00771 (testing) 0.00892, accuracy: (training)90.625 (testing) 73.04688\n",
      "[epoch 00153]  loss: (training)0.0077 (testing) 0.00893, accuracy: (training)90.625 (testing) 73.04688\n",
      "[epoch 00154]  loss: (training)0.0077 (testing) 0.00893, accuracy: (training)90.625 (testing) 73.04688\n",
      "[epoch 00155]  loss: (training)0.00769 (testing) 0.00894, accuracy: (training)90.71691 (testing) 73.04688\n",
      "[epoch 00156]  loss: (training)0.00768 (testing) 0.00894, accuracy: (training)90.53309 (testing) 73.04688\n",
      "[epoch 00157]  loss: (training)0.00768 (testing) 0.00894, accuracy: (training)90.625 (testing) 72.65625\n",
      "[epoch 00158]  loss: (training)0.00767 (testing) 0.00894, accuracy: (training)90.625 (testing) 72.65625\n",
      "[epoch 00159]  loss: (training)0.00766 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00160]  loss: (training)0.00765 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00161]  loss: (training)0.00765 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00162]  loss: (training)0.00764 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00163]  loss: (training)0.00763 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00164]  loss: (training)0.00763 (testing) 0.00895, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00165]  loss: (training)0.00762 (testing) 0.00894, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00166]  loss: (training)0.00761 (testing) 0.00894, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00167]  loss: (training)0.0076 (testing) 0.00893, accuracy: (training)90.53309 (testing) 72.65625\n",
      "[epoch 00168]  loss: (training)0.0076 (testing) 0.00894, accuracy: (training)90.625 (testing) 72.65625\n",
      "[epoch 00169]  loss: (training)0.00759 (testing) 0.00892, accuracy: (training)90.625 (testing) 72.65625\n",
      "[epoch 00170]  loss: (training)0.00758 (testing) 0.00892, accuracy: (training)90.80882 (testing) 72.65625\n",
      "[epoch 00171]  loss: (training)0.00757 (testing) 0.0089, accuracy: (training)90.80882 (testing) 73.04688\n",
      "[epoch 00172]  loss: (training)0.00756 (testing) 0.00888, accuracy: (training)91.17647 (testing) 73.4375\n",
      "[epoch 00173]  loss: (training)0.00755 (testing) 0.00887, accuracy: (training)91.17647 (testing) 73.4375\n",
      "[epoch 00174]  loss: (training)0.00754 (testing) 0.00885, accuracy: (training)91.26838 (testing) 73.4375\n",
      "[epoch 00175]  loss: (training)0.00752 (testing) 0.00882, accuracy: (training)91.26838 (testing) 73.82812\n",
      "[epoch 00176]  loss: (training)0.00751 (testing) 0.00881, accuracy: (training)91.26838 (testing) 73.82812\n",
      "[epoch 00177]  loss: (training)0.0075 (testing) 0.00879, accuracy: (training)91.36029 (testing) 73.82812\n",
      "[epoch 00178]  loss: (training)0.00749 (testing) 0.00877, accuracy: (training)91.36029 (testing) 73.82812\n",
      "[epoch 00179]  loss: (training)0.00748 (testing) 0.00875, accuracy: (training)91.54412 (testing) 73.82812\n",
      "[epoch 00180]  loss: (training)0.00746 (testing) 0.00874, accuracy: (training)91.63603 (testing) 74.21875\n",
      "[epoch 00181]  loss: (training)0.00745 (testing) 0.00872, accuracy: (training)91.72794 (testing) 74.21875\n",
      "[epoch 00182]  loss: (training)0.00744 (testing) 0.00871, accuracy: (training)91.72794 (testing) 74.21875\n",
      "[epoch 00183]  loss: (training)0.00743 (testing) 0.00869, accuracy: (training)92.00368 (testing) 74.21875\n",
      "[epoch 00184]  loss: (training)0.00741 (testing) 0.00869, accuracy: (training)92.00368 (testing) 74.21875\n",
      "[epoch 00185]  loss: (training)0.0074 (testing) 0.00869, accuracy: (training)92.09559 (testing) 74.21875\n",
      "[epoch 00186]  loss: (training)0.00739 (testing) 0.00866, accuracy: (training)92.1875 (testing) 74.21875\n",
      "[epoch 00187]  loss: (training)0.00738 (testing) 0.00866, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00188]  loss: (training)0.00737 (testing) 0.00867, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00189]  loss: (training)0.00736 (testing) 0.00865, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00190]  loss: (training)0.00735 (testing) 0.00864, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00191]  loss: (training)0.00734 (testing) 0.00865, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00192]  loss: (training)0.00733 (testing) 0.00865, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00193]  loss: (training)0.00732 (testing) 0.00864, accuracy: (training)92.27941 (testing) 74.21875\n",
      "[epoch 00194]  loss: (training)0.00731 (testing) 0.00862, accuracy: (training)92.46324 (testing) 74.21875\n",
      "[epoch 00195]  loss: (training)0.0073 (testing) 0.00861, accuracy: (training)92.64706 (testing) 74.21875\n",
      "[epoch 00196]  loss: (training)0.00729 (testing) 0.00859, accuracy: (training)92.73897 (testing) 74.21875\n",
      "[epoch 00197]  loss: (training)0.00728 (testing) 0.00855, accuracy: (training)92.92279 (testing) 74.60938\n",
      "[epoch 00198]  loss: (training)0.00726 (testing) 0.00853, accuracy: (training)93.10662 (testing) 75.0\n",
      "[epoch 00199]  loss: (training)0.00725 (testing) 0.0085, accuracy: (training)93.38235 (testing) 75.0\n",
      "[epoch 00200]  loss: (training)0.00724 (testing) 0.00847, accuracy: (training)93.38235 (testing) 75.0\n",
      "[epoch 00201]  loss: (training)0.00722 (testing) 0.00845, accuracy: (training)93.65809 (testing) 75.0\n",
      "[epoch 00202]  loss: (training)0.00721 (testing) 0.00844, accuracy: (training)93.75 (testing) 75.0\n",
      "[epoch 00203]  loss: (training)0.0072 (testing) 0.0084, accuracy: (training)93.75 (testing) 75.39062\n",
      "[epoch 00204]  loss: (training)0.00719 (testing) 0.00839, accuracy: (training)93.75 (testing) 75.39062\n",
      "[epoch 00205]  loss: (training)0.00718 (testing) 0.00837, accuracy: (training)93.93382 (testing) 76.17188\n",
      "[epoch 00206]  loss: (training)0.00716 (testing) 0.00836, accuracy: (training)94.02574 (testing) 76.95312\n",
      "[epoch 00207]  loss: (training)0.00715 (testing) 0.00834, accuracy: (training)94.11765 (testing) 77.34375\n",
      "[epoch 00208]  loss: (training)0.00714 (testing) 0.00834, accuracy: (training)94.11765 (testing) 77.34375\n",
      "[epoch 00209]  loss: (training)0.00713 (testing) 0.00835, accuracy: (training)94.20956 (testing) 77.34375\n",
      "[epoch 00210]  loss: (training)0.00712 (testing) 0.00833, accuracy: (training)94.30147 (testing) 77.34375\n",
      "[epoch 00211]  loss: (training)0.00711 (testing) 0.00834, accuracy: (training)94.30147 (testing) 76.95312\n",
      "[epoch 00212]  loss: (training)0.0071 (testing) 0.00834, accuracy: (training)94.39338 (testing) 76.95312\n",
      "[epoch 00213]  loss: (training)0.00709 (testing) 0.00835, accuracy: (training)94.39338 (testing) 76.95312\n",
      "[epoch 00214]  loss: (training)0.00708 (testing) 0.00835, accuracy: (training)94.39338 (testing) 76.95312\n",
      "[epoch 00215]  loss: (training)0.00707 (testing) 0.00838, accuracy: (training)94.39338 (testing) 75.78125\n",
      "[epoch 00216]  loss: (training)0.00707 (testing) 0.00839, accuracy: (training)94.39338 (testing) 75.78125\n",
      "[epoch 00217]  loss: (training)0.00706 (testing) 0.0084, accuracy: (training)94.48529 (testing) 75.39062\n",
      "[epoch 00218]  loss: (training)0.00705 (testing) 0.00844, accuracy: (training)94.66912 (testing) 75.39062\n",
      "[epoch 00219]  loss: (training)0.00705 (testing) 0.00846, accuracy: (training)94.57721 (testing) 75.39062\n",
      "[epoch 00220]  loss: (training)0.00705 (testing) 0.00848, accuracy: (training)94.57721 (testing) 75.0\n",
      "[epoch 00221]  loss: (training)0.00704 (testing) 0.00852, accuracy: (training)94.57721 (testing) 74.21875\n",
      "[epoch 00222]  loss: (training)0.00704 (testing) 0.00854, accuracy: (training)94.66912 (testing) 74.21875\n",
      "[epoch 00223]  loss: (training)0.00703 (testing) 0.00858, accuracy: (training)94.66912 (testing) 73.82812\n",
      "[epoch 00224]  loss: (training)0.00703 (testing) 0.00861, accuracy: (training)94.66912 (testing) 73.82812\n",
      "[epoch 00225]  loss: (training)0.00703 (testing) 0.00864, accuracy: (training)94.66912 (testing) 73.82812\n",
      "[epoch 00226]  loss: (training)0.00703 (testing) 0.00875, accuracy: (training)94.66912 (testing) 73.04688\n",
      "[epoch 00227]  loss: (training)0.00703 (testing) 0.00872, accuracy: (training)94.66912 (testing) 73.04688\n",
      "[epoch 00228]  loss: (training)0.00702 (testing) 0.00881, accuracy: (training)94.57721 (testing) 72.65625\n",
      "[epoch 00229]  loss: (training)0.00703 (testing) 0.00884, accuracy: (training)94.66912 (testing) 72.65625\n",
      "[epoch 00230]  loss: (training)0.00703 (testing) 0.0089, accuracy: (training)94.76103 (testing) 73.04688\n",
      "[epoch 00231]  loss: (training)0.00703 (testing) 0.00902, accuracy: (training)94.66912 (testing) 73.04688\n",
      "[epoch 00232]  loss: (training)0.00704 (testing) 0.00907, accuracy: (training)94.48529 (testing) 73.04688\n",
      "[epoch 00233]  loss: (training)0.00704 (testing) 0.00911, accuracy: (training)94.48529 (testing) 72.65625\n",
      "[epoch 00234]  loss: (training)0.00705 (testing) 0.00931, accuracy: (training)94.48529 (testing) 70.70312\n",
      "[epoch 00235]  loss: (training)0.00707 (testing) 0.00938, accuracy: (training)94.20956 (testing) 70.3125\n",
      "[epoch 00236]  loss: (training)0.00708 (testing) 0.00949, accuracy: (training)94.20956 (testing) 69.53125\n",
      "[epoch 00237]  loss: (training)0.0071 (testing) 0.00962, accuracy: (training)94.02574 (testing) 69.14062\n",
      "[epoch 00238]  loss: (training)0.00713 (testing) 0.00967, accuracy: (training)93.84191 (testing) 68.75\n",
      "[epoch 00239]  loss: (training)0.00714 (testing) 0.00977, accuracy: (training)93.75 (testing) 68.35938\n",
      "[epoch 00240]  loss: (training)0.00716 (testing) 0.00967, accuracy: (training)93.56618 (testing) 69.14062\n",
      "[epoch 00241]  loss: (training)0.00715 (testing) 0.00967, accuracy: (training)93.56618 (testing) 69.14062\n",
      "[epoch 00242]  loss: (training)0.00715 (testing) 0.00949, accuracy: (training)93.65809 (testing) 69.92188\n",
      "[epoch 00243]  loss: (training)0.00711 (testing) 0.00947, accuracy: (training)93.93382 (testing) 69.92188\n",
      "[epoch 00244]  loss: (training)0.00711 (testing) 0.00941, accuracy: (training)94.11765 (testing) 69.92188\n",
      "[epoch 00245]  loss: (training)0.0071 (testing) 0.00933, accuracy: (training)94.30147 (testing) 70.3125\n",
      "[epoch 00246]  loss: (training)0.00708 (testing) 0.00932, accuracy: (training)94.39338 (testing) 70.70312\n",
      "[epoch 00247]  loss: (training)0.00708 (testing) 0.00929, accuracy: (training)94.39338 (testing) 71.09375\n",
      "[epoch 00248]  loss: (training)0.00707 (testing) 0.0093, accuracy: (training)94.30147 (testing) 71.09375\n",
      "[epoch 00249]  loss: (training)0.00707 (testing) 0.00922, accuracy: (training)94.39338 (testing) 71.09375\n",
      "[epoch 00250]  loss: (training)0.00706 (testing) 0.00924, accuracy: (training)94.66912 (testing) 71.09375\n",
      "[epoch 00251]  loss: (training)0.00705 (testing) 0.00917, accuracy: (training)94.57721 (testing) 71.48438\n",
      "[epoch 00252]  loss: (training)0.00704 (testing) 0.00917, accuracy: (training)94.76103 (testing) 71.48438\n",
      "[epoch 00253]  loss: (training)0.00704 (testing) 0.00914, accuracy: (training)94.76103 (testing) 71.875\n",
      "[epoch 00254]  loss: (training)0.00703 (testing) 0.00916, accuracy: (training)94.94485 (testing) 71.48438\n",
      "[epoch 00255]  loss: (training)0.00703 (testing) 0.00907, accuracy: (training)95.03676 (testing) 72.26562\n",
      "[epoch 00256]  loss: (training)0.00702 (testing) 0.00906, accuracy: (training)95.22059 (testing) 72.26562\n",
      "[epoch 00257]  loss: (training)0.00701 (testing) 0.00901, accuracy: (training)95.22059 (testing) 72.65625\n",
      "[epoch 00258]  loss: (training)0.007 (testing) 0.00901, accuracy: (training)95.3125 (testing) 72.26562\n",
      "[epoch 00259]  loss: (training)0.007 (testing) 0.00896, accuracy: (training)95.3125 (testing) 72.65625\n",
      "[epoch 00260]  loss: (training)0.00699 (testing) 0.00894, accuracy: (training)95.3125 (testing) 72.65625\n",
      "[epoch 00261]  loss: (training)0.00698 (testing) 0.0089, accuracy: (training)95.3125 (testing) 73.82812\n",
      "[epoch 00262]  loss: (training)0.00698 (testing) 0.00887, accuracy: (training)95.40441 (testing) 73.82812\n",
      "[epoch 00263]  loss: (training)0.00697 (testing) 0.00884, accuracy: (training)95.40441 (testing) 73.82812\n",
      "[epoch 00264]  loss: (training)0.00696 (testing) 0.00881, accuracy: (training)95.58824 (testing) 74.21875\n",
      "[epoch 00265]  loss: (training)0.00695 (testing) 0.0088, accuracy: (training)95.58824 (testing) 74.60938\n",
      "[epoch 00266]  loss: (training)0.00695 (testing) 0.00877, accuracy: (training)95.58824 (testing) 75.39062\n",
      "[epoch 00267]  loss: (training)0.00694 (testing) 0.00876, accuracy: (training)95.58824 (testing) 75.39062\n",
      "[epoch 00268]  loss: (training)0.00693 (testing) 0.00874, accuracy: (training)95.58824 (testing) 75.39062\n",
      "[epoch 00269]  loss: (training)0.00693 (testing) 0.0087, accuracy: (training)95.58824 (testing) 75.78125\n",
      "[epoch 00270]  loss: (training)0.00692 (testing) 0.0087, accuracy: (training)95.77206 (testing) 75.78125\n",
      "[epoch 00271]  loss: (training)0.00692 (testing) 0.00867, accuracy: (training)95.77206 (testing) 76.17188\n",
      "[epoch 00272]  loss: (training)0.00691 (testing) 0.00864, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00273]  loss: (training)0.00691 (testing) 0.00863, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00274]  loss: (training)0.0069 (testing) 0.00859, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00275]  loss: (training)0.0069 (testing) 0.00861, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00276]  loss: (training)0.00689 (testing) 0.00856, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00277]  loss: (training)0.00689 (testing) 0.00861, accuracy: (training)96.04779 (testing) 76.17188\n",
      "[epoch 00278]  loss: (training)0.00689 (testing) 0.00852, accuracy: (training)96.04779 (testing) 76.17188\n",
      "[epoch 00279]  loss: (training)0.00688 (testing) 0.00858, accuracy: (training)95.58824 (testing) 76.17188\n",
      "[epoch 00280]  loss: (training)0.00689 (testing) 0.00847, accuracy: (training)95.95588 (testing) 76.17188\n",
      "[epoch 00281]  loss: (training)0.00688 (testing) 0.00851, accuracy: (training)95.86397 (testing) 76.17188\n",
      "[epoch 00282]  loss: (training)0.00688 (testing) 0.00843, accuracy: (training)95.86397 (testing) 75.78125\n",
      "[epoch 00283]  loss: (training)0.00689 (testing) 0.00835, accuracy: (training)95.68015 (testing) 75.78125\n",
      "[epoch 00284]  loss: (training)0.00689 (testing) 0.00832, accuracy: (training)95.58824 (testing) 75.78125\n",
      "[epoch 00285]  loss: (training)0.00691 (testing) 0.00814, accuracy: (training)95.40441 (testing) 77.34375\n",
      "[epoch 00286]  loss: (training)0.00689 (testing) 0.00815, accuracy: (training)95.58824 (testing) 77.34375\n",
      "[epoch 00287]  loss: (training)0.00692 (testing) 0.00794, accuracy: (training)95.40441 (testing) 80.07812\n",
      "[epoch 00288]  loss: (training)0.00689 (testing) 0.0079, accuracy: (training)95.49632 (testing) 80.46875\n",
      "[epoch 00289]  loss: (training)0.00687 (testing) 0.00783, accuracy: (training)95.77206 (testing) 81.25\n",
      "[epoch 00290]  loss: (training)0.00686 (testing) 0.0078, accuracy: (training)95.95588 (testing) 81.25\n",
      "[epoch 00291]  loss: (training)0.00684 (testing) 0.00778, accuracy: (training)96.13971 (testing) 81.64062\n",
      "[epoch 00292]  loss: (training)0.00683 (testing) 0.00776, accuracy: (training)96.13971 (testing) 81.64062\n",
      "[epoch 00293]  loss: (training)0.00683 (testing) 0.00777, accuracy: (training)96.13971 (testing) 81.64062\n",
      "[epoch 00294]  loss: (training)0.00682 (testing) 0.00778, accuracy: (training)96.04779 (testing) 81.25\n",
      "[epoch 00295]  loss: (training)0.00681 (testing) 0.00782, accuracy: (training)96.13971 (testing) 81.25\n",
      "[epoch 00296]  loss: (training)0.00681 (testing) 0.00782, accuracy: (training)96.04779 (testing) 81.25\n",
      "[epoch 00297]  loss: (training)0.0068 (testing) 0.00788, accuracy: (training)96.23162 (testing) 80.46875\n",
      "[epoch 00298]  loss: (training)0.0068 (testing) 0.00784, accuracy: (training)96.13971 (testing) 80.46875\n",
      "[epoch 00299]  loss: (training)0.00679 (testing) 0.00789, accuracy: (training)96.13971 (testing) 79.6875\n",
      "[epoch 00300]  loss: (training)0.00679 (testing) 0.00788, accuracy: (training)96.32353 (testing) 80.07812\n",
      "[epoch 00301]  loss: (training)0.00679 (testing) 0.00787, accuracy: (training)96.23162 (testing) 80.46875\n",
      "[epoch 00302]  loss: (training)0.00679 (testing) 0.00782, accuracy: (training)96.23162 (testing) 80.85938\n",
      "[epoch 00303]  loss: (training)0.00677 (testing) 0.00785, accuracy: (training)96.32353 (testing) 80.46875\n",
      "[epoch 00304]  loss: (training)0.00677 (testing) 0.00779, accuracy: (training)96.32353 (testing) 82.03125\n",
      "[epoch 00305]  loss: (training)0.00676 (testing) 0.00782, accuracy: (training)96.41544 (testing) 81.25\n",
      "[epoch 00306]  loss: (training)0.00676 (testing) 0.00777, accuracy: (training)96.50735 (testing) 82.03125\n",
      "[epoch 00307]  loss: (training)0.00675 (testing) 0.00778, accuracy: (training)96.59926 (testing) 81.64062\n",
      "[epoch 00308]  loss: (training)0.00675 (testing) 0.00775, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00309]  loss: (training)0.00674 (testing) 0.00775, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00310]  loss: (training)0.00675 (testing) 0.00773, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00311]  loss: (training)0.00674 (testing) 0.00773, accuracy: (training)96.59926 (testing) 82.03125\n",
      "[epoch 00312]  loss: (training)0.00674 (testing) 0.00772, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00313]  loss: (training)0.00673 (testing) 0.00772, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00314]  loss: (training)0.00674 (testing) 0.00772, accuracy: (training)96.69118 (testing) 82.03125\n",
      "[epoch 00315]  loss: (training)0.00673 (testing) 0.00773, accuracy: (training)96.875 (testing) 81.64062\n",
      "[epoch 00316]  loss: (training)0.00673 (testing) 0.00772, accuracy: (training)96.875 (testing) 82.03125\n",
      "[epoch 00317]  loss: (training)0.00673 (testing) 0.00772, accuracy: (training)96.78309 (testing) 82.03125\n",
      "[epoch 00318]  loss: (training)0.00673 (testing) 0.00771, accuracy: (training)96.875 (testing) 82.03125\n",
      "[epoch 00319]  loss: (training)0.00673 (testing) 0.00772, accuracy: (training)96.78309 (testing) 82.03125\n",
      "[epoch 00320]  loss: (training)0.00672 (testing) 0.00771, accuracy: (training)96.875 (testing) 82.03125\n",
      "[epoch 00321]  loss: (training)0.00672 (testing) 0.00771, accuracy: (training)96.875 (testing) 81.64062\n",
      "[epoch 00322]  loss: (training)0.00672 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.64062\n",
      "[epoch 00323]  loss: (training)0.00671 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.64062\n",
      "[epoch 00324]  loss: (training)0.00671 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.64062\n",
      "[epoch 00325]  loss: (training)0.00671 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.64062\n",
      "[epoch 00326]  loss: (training)0.0067 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.64062\n",
      "[epoch 00327]  loss: (training)0.0067 (testing) 0.00771, accuracy: (training)96.96691 (testing) 81.25\n",
      "[epoch 00328]  loss: (training)0.0067 (testing) 0.00771, accuracy: (training)96.875 (testing) 80.85938\n",
      "[epoch 00329]  loss: (training)0.0067 (testing) 0.00772, accuracy: (training)96.78309 (testing) 80.85938\n",
      "[epoch 00330]  loss: (training)0.00669 (testing) 0.00772, accuracy: (training)96.78309 (testing) 80.85938\n",
      "[epoch 00331]  loss: (training)0.00669 (testing) 0.00773, accuracy: (training)96.78309 (testing) 80.46875\n",
      "[epoch 00332]  loss: (training)0.00669 (testing) 0.00773, accuracy: (training)96.875 (testing) 80.07812\n",
      "[epoch 00333]  loss: (training)0.0067 (testing) 0.00777, accuracy: (training)96.78309 (testing) 80.46875\n",
      "[epoch 00334]  loss: (training)0.0067 (testing) 0.00787, accuracy: (training)96.69118 (testing) 80.07812\n",
      "[epoch 00335]  loss: (training)0.00671 (testing) 0.00871, accuracy: (training)96.32353 (testing) 74.21875\n",
      "[epoch 00336]  loss: (training)0.00693 (testing) 0.01156, accuracy: (training)94.85294 (testing) 57.42188\n",
      "[epoch 00337]  loss: (training)0.00834 (testing) 0.00797, accuracy: (training)85.56985 (testing) 79.29688\n",
      "[epoch 00338]  loss: (training)0.00682 (testing) 0.01113, accuracy: (training)95.86397 (testing) 59.375\n",
      "[epoch 00339]  loss: (training)0.00758 (testing) 0.01162, accuracy: (training)89.98162 (testing) 57.03125\n",
      "[epoch 00340]  loss: (training)0.00889 (testing) 0.01084, accuracy: (training)81.25 (testing) 60.54688\n",
      "[epoch 00341]  loss: (training)0.00751 (testing) 0.01136, accuracy: (training)90.44118 (testing) 58.20312\n",
      "[epoch 00342]  loss: (training)0.00784 (testing) 0.01125, accuracy: (training)88.41912 (testing) 59.375\n",
      "[epoch 00343]  loss: (training)0.0076 (testing) 0.00975, accuracy: (training)90.53309 (testing) 68.75\n",
      "[epoch 00344]  loss: (training)0.00726 (testing) 0.01099, accuracy: (training)92.27941 (testing) 59.76562\n",
      "[epoch 00345]  loss: (training)0.0075 (testing) 0.01089, accuracy: (training)90.34926 (testing) 60.54688\n",
      "[epoch 00346]  loss: (training)0.0074 (testing) 0.01056, accuracy: (training)92.00368 (testing) 63.28125\n",
      "[epoch 00347]  loss: (training)0.00743 (testing) 0.01059, accuracy: (training)91.08456 (testing) 63.28125\n",
      "[epoch 00348]  loss: (training)0.00739 (testing) 0.01015, accuracy: (training)92.09559 (testing) 65.625\n",
      "[epoch 00349]  loss: (training)0.00734 (testing) 0.00945, accuracy: (training)92.46324 (testing) 70.3125\n",
      "[epoch 00350]  loss: (training)0.0071 (testing) 0.01052, accuracy: (training)93.75 (testing) 63.67188\n",
      "[epoch 00351]  loss: (training)0.00738 (testing) 0.00897, accuracy: (training)92.27941 (testing) 73.4375\n",
      "[epoch 00352]  loss: (training)0.00706 (testing) 0.01036, accuracy: (training)94.02574 (testing) 64.84375\n",
      "[epoch 00353]  loss: (training)0.0074 (testing) 0.00887, accuracy: (training)92.37132 (testing) 73.82812\n",
      "[epoch 00354]  loss: (training)0.00706 (testing) 0.0102, accuracy: (training)94.11765 (testing) 65.625\n",
      "[epoch 00355]  loss: (training)0.00736 (testing) 0.00862, accuracy: (training)92.64706 (testing) 75.39062\n",
      "[epoch 00356]  loss: (training)0.00704 (testing) 0.01011, accuracy: (training)94.30147 (testing) 66.01562\n",
      "[epoch 00357]  loss: (training)0.00733 (testing) 0.00854, accuracy: (training)92.83088 (testing) 76.5625\n",
      "[epoch 00358]  loss: (training)0.00702 (testing) 0.00977, accuracy: (training)94.57721 (testing) 68.75\n",
      "[epoch 00359]  loss: (training)0.00721 (testing) 0.00846, accuracy: (training)93.65809 (testing) 76.95312\n",
      "[epoch 00360]  loss: (training)0.00699 (testing) 0.00968, accuracy: (training)94.66912 (testing) 68.75\n",
      "[epoch 00361]  loss: (training)0.00715 (testing) 0.00845, accuracy: (training)94.02574 (testing) 76.95312\n",
      "[epoch 00362]  loss: (training)0.00697 (testing) 0.00943, accuracy: (training)94.85294 (testing) 69.92188\n",
      "[epoch 00363]  loss: (training)0.00706 (testing) 0.0092, accuracy: (training)94.02574 (testing) 71.48438\n",
      "[epoch 00364]  loss: (training)0.00701 (testing) 0.00923, accuracy: (training)94.66912 (testing) 71.48438\n",
      "[epoch 00365]  loss: (training)0.00704 (testing) 0.00984, accuracy: (training)94.20956 (testing) 68.35938\n",
      "[epoch 00366]  loss: (training)0.00719 (testing) 0.00865, accuracy: (training)93.93382 (testing) 75.0\n",
      "[epoch 00367]  loss: (training)0.00702 (testing) 0.00857, accuracy: (training)94.85294 (testing) 75.78125\n",
      "[epoch 00368]  loss: (training)0.00699 (testing) 0.00846, accuracy: (training)95.12868 (testing) 77.34375\n",
      "[epoch 00369]  loss: (training)0.00694 (testing) 0.00861, accuracy: (training)95.12868 (testing) 75.39062\n",
      "[epoch 00370]  loss: (training)0.00698 (testing) 0.00838, accuracy: (training)95.12868 (testing) 77.34375\n",
      "[epoch 00371]  loss: (training)0.00694 (testing) 0.00993, accuracy: (training)95.12868 (testing) 67.57812\n",
      "[epoch 00372]  loss: (training)0.00716 (testing) 0.0086, accuracy: (training)94.02574 (testing) 75.39062\n",
      "[epoch 00373]  loss: (training)0.00696 (testing) 0.00857, accuracy: (training)95.12868 (testing) 75.39062\n",
      "[epoch 00374]  loss: (training)0.00697 (testing) 0.00844, accuracy: (training)95.12868 (testing) 77.34375\n",
      "[epoch 00375]  loss: (training)0.00692 (testing) 0.00929, accuracy: (training)95.3125 (testing) 71.48438\n",
      "[epoch 00376]  loss: (training)0.00699 (testing) 0.00876, accuracy: (training)94.94485 (testing) 75.0\n",
      "[epoch 00377]  loss: (training)0.00698 (testing) 0.00849, accuracy: (training)95.22059 (testing) 76.5625\n",
      "[epoch 00378]  loss: (training)0.00692 (testing) 0.00917, accuracy: (training)95.3125 (testing) 72.26562\n",
      "[epoch 00379]  loss: (training)0.00698 (testing) 0.00856, accuracy: (training)95.12868 (testing) 75.39062\n",
      "[epoch 00380]  loss: (training)0.00693 (testing) 0.00866, accuracy: (training)95.40441 (testing) 75.0\n",
      "[epoch 00381]  loss: (training)0.00696 (testing) 0.00853, accuracy: (training)95.40441 (testing) 76.17188\n",
      "[epoch 00382]  loss: (training)0.00691 (testing) 0.00869, accuracy: (training)95.40441 (testing) 75.0\n",
      "[epoch 00383]  loss: (training)0.00696 (testing) 0.00848, accuracy: (training)95.3125 (testing) 76.5625\n",
      "[epoch 00384]  loss: (training)0.00692 (testing) 0.00929, accuracy: (training)95.12868 (testing) 71.48438\n",
      "[epoch 00385]  loss: (training)0.00698 (testing) 0.00869, accuracy: (training)95.22059 (testing) 75.0\n",
      "[epoch 00386]  loss: (training)0.007 (testing) 0.00842, accuracy: (training)95.03676 (testing) 77.34375\n",
      "[epoch 00387]  loss: (training)0.00696 (testing) 0.00926, accuracy: (training)94.85294 (testing) 71.48438\n",
      "[epoch 00388]  loss: (training)0.007 (testing) 0.00843, accuracy: (training)95.12868 (testing) 77.34375\n",
      "[epoch 00389]  loss: (training)0.00694 (testing) 0.00907, accuracy: (training)94.94485 (testing) 72.65625\n",
      "[epoch 00390]  loss: (training)0.00707 (testing) 0.00824, accuracy: (training)94.66912 (testing) 77.73438\n",
      "[epoch 00391]  loss: (training)0.00692 (testing) 0.00902, accuracy: (training)95.03676 (testing) 73.04688\n",
      "[epoch 00392]  loss: (training)0.00706 (testing) 0.00817, accuracy: (training)94.57721 (testing) 78.125\n",
      "[epoch 00393]  loss: (training)0.00692 (testing) 0.00919, accuracy: (training)95.12868 (testing) 72.26562\n",
      "[epoch 00394]  loss: (training)0.00706 (testing) 0.00814, accuracy: (training)94.48529 (testing) 78.51562\n",
      "[epoch 00395]  loss: (training)0.00693 (testing) 0.00945, accuracy: (training)95.22059 (testing) 69.92188\n",
      "[epoch 00396]  loss: (training)0.00706 (testing) 0.00809, accuracy: (training)94.57721 (testing) 78.51562\n",
      "[epoch 00397]  loss: (training)0.00692 (testing) 0.0091, accuracy: (training)95.3125 (testing) 72.65625\n",
      "[epoch 00398]  loss: (training)0.00703 (testing) 0.00804, accuracy: (training)94.66912 (testing) 79.29688\n",
      "[epoch 00399]  loss: (training)0.00691 (testing) 0.00889, accuracy: (training)95.3125 (testing) 74.21875\n",
      "[epoch 00400]  loss: (training)0.00701 (testing) 0.00801, accuracy: (training)94.57721 (testing) 80.07812\n",
      "[epoch 00401]  loss: (training)0.00689 (testing) 0.00849, accuracy: (training)95.49632 (testing) 76.17188\n",
      "[epoch 00402]  loss: (training)0.00698 (testing) 0.00812, accuracy: (training)95.03676 (testing) 78.125\n",
      "[epoch 00403]  loss: (training)0.00691 (testing) 0.00804, accuracy: (training)95.49632 (testing) 79.6875\n",
      "[epoch 00404]  loss: (training)0.00687 (testing) 0.00781, accuracy: (training)95.58824 (testing) 80.46875\n",
      "[epoch 00405]  loss: (training)0.00687 (testing) 0.00781, accuracy: (training)95.68015 (testing) 80.46875\n",
      "[epoch 00406]  loss: (training)0.00685 (testing) 0.00775, accuracy: (training)95.68015 (testing) 81.64062\n",
      "[epoch 00407]  loss: (training)0.00686 (testing) 0.00775, accuracy: (training)95.68015 (testing) 81.64062\n",
      "[epoch 00408]  loss: (training)0.00682 (testing) 0.00771, accuracy: (training)95.95588 (testing) 82.03125\n",
      "[epoch 00409]  loss: (training)0.00682 (testing) 0.00771, accuracy: (training)95.95588 (testing) 82.03125\n",
      "[epoch 00410]  loss: (training)0.0068 (testing) 0.00768, accuracy: (training)96.13971 (testing) 82.03125\n",
      "[epoch 00411]  loss: (training)0.00679 (testing) 0.00764, accuracy: (training)96.32353 (testing) 82.42188\n",
      "[epoch 00412]  loss: (training)0.00679 (testing) 0.00764, accuracy: (training)96.41544 (testing) 82.42188\n",
      "[epoch 00413]  loss: (training)0.00677 (testing) 0.00759, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00414]  loss: (training)0.00678 (testing) 0.0076, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00415]  loss: (training)0.00676 (testing) 0.00756, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00416]  loss: (training)0.00677 (testing) 0.00761, accuracy: (training)96.23162 (testing) 82.42188\n",
      "[epoch 00417]  loss: (training)0.00674 (testing) 0.00751, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00418]  loss: (training)0.00678 (testing) 0.00764, accuracy: (training)96.04779 (testing) 82.42188\n",
      "[epoch 00419]  loss: (training)0.00675 (testing) 0.00753, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00420]  loss: (training)0.00674 (testing) 0.00754, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00421]  loss: (training)0.00672 (testing) 0.00754, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00422]  loss: (training)0.00672 (testing) 0.00756, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00423]  loss: (training)0.00674 (testing) 0.00763, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00424]  loss: (training)0.00677 (testing) 0.00766, accuracy: (training)96.41544 (testing) 82.42188\n",
      "[epoch 00425]  loss: (training)0.00674 (testing) 0.00773, accuracy: (training)96.59926 (testing) 82.03125\n",
      "[epoch 00426]  loss: (training)0.00675 (testing) 0.00804, accuracy: (training)96.41544 (testing) 78.90625\n",
      "[epoch 00427]  loss: (training)0.00678 (testing) 0.00776, accuracy: (training)96.32353 (testing) 81.25\n",
      "[epoch 00428]  loss: (training)0.00682 (testing) 0.00974, accuracy: (training)95.86397 (testing) 68.75\n",
      "[epoch 00429]  loss: (training)0.00705 (testing) 0.01023, accuracy: (training)94.20956 (testing) 65.23438\n",
      "[epoch 00430]  loss: (training)0.00714 (testing) 0.00837, accuracy: (training)93.56618 (testing) 77.73438\n",
      "[epoch 00431]  loss: (training)0.00692 (testing) 0.01047, accuracy: (training)94.85294 (testing) 63.67188\n",
      "[epoch 00432]  loss: (training)0.00723 (testing) 0.00817, accuracy: (training)93.56618 (testing) 78.125\n",
      "[epoch 00433]  loss: (training)0.00687 (testing) 0.00981, accuracy: (training)95.22059 (testing) 68.35938\n",
      "[epoch 00434]  loss: (training)0.00704 (testing) 0.00876, accuracy: (training)94.20956 (testing) 74.60938\n",
      "[epoch 00435]  loss: (training)0.00691 (testing) 0.00849, accuracy: (training)95.40441 (testing) 76.5625\n",
      "[epoch 00436]  loss: (training)0.00687 (testing) 0.00889, accuracy: (training)95.3125 (testing) 74.21875\n",
      "[epoch 00437]  loss: (training)0.00693 (testing) 0.00841, accuracy: (training)94.94485 (testing) 76.5625\n",
      "[epoch 00438]  loss: (training)0.00686 (testing) 0.00874, accuracy: (training)95.40441 (testing) 74.60938\n",
      "[epoch 00439]  loss: (training)0.0069 (testing) 0.00837, accuracy: (training)95.77206 (testing) 77.34375\n",
      "[epoch 00440]  loss: (training)0.00685 (testing) 0.00872, accuracy: (training)95.68015 (testing) 74.60938\n",
      "[epoch 00441]  loss: (training)0.00691 (testing) 0.00837, accuracy: (training)95.68015 (testing) 77.34375\n",
      "[epoch 00442]  loss: (training)0.00683 (testing) 0.00848, accuracy: (training)95.95588 (testing) 76.5625\n",
      "[epoch 00443]  loss: (training)0.00686 (testing) 0.00832, accuracy: (training)95.86397 (testing) 77.73438\n",
      "[epoch 00444]  loss: (training)0.00684 (testing) 0.00825, accuracy: (training)96.13971 (testing) 78.125\n",
      "[epoch 00445]  loss: (training)0.00684 (testing) 0.00817, accuracy: (training)96.13971 (testing) 78.51562\n",
      "[epoch 00446]  loss: (training)0.00683 (testing) 0.00813, accuracy: (training)96.23162 (testing) 78.51562\n",
      "[epoch 00447]  loss: (training)0.00684 (testing) 0.00807, accuracy: (training)96.13971 (testing) 79.29688\n",
      "[epoch 00448]  loss: (training)0.00682 (testing) 0.00802, accuracy: (training)96.23162 (testing) 79.6875\n",
      "[epoch 00449]  loss: (training)0.00682 (testing) 0.008, accuracy: (training)96.32353 (testing) 79.6875\n",
      "[epoch 00450]  loss: (training)0.00682 (testing) 0.00796, accuracy: (training)96.23162 (testing) 79.6875\n",
      "[epoch 00451]  loss: (training)0.00681 (testing) 0.00794, accuracy: (training)96.32353 (testing) 79.29688\n",
      "[epoch 00452]  loss: (training)0.00681 (testing) 0.00792, accuracy: (training)96.32353 (testing) 79.29688\n",
      "[epoch 00453]  loss: (training)0.0068 (testing) 0.00788, accuracy: (training)96.32353 (testing) 80.07812\n",
      "[epoch 00454]  loss: (training)0.0068 (testing) 0.00787, accuracy: (training)96.32353 (testing) 80.07812\n",
      "[epoch 00455]  loss: (training)0.0068 (testing) 0.00785, accuracy: (training)96.41544 (testing) 80.46875\n",
      "[epoch 00456]  loss: (training)0.0068 (testing) 0.00782, accuracy: (training)96.32353 (testing) 80.46875\n",
      "[epoch 00457]  loss: (training)0.0068 (testing) 0.00776, accuracy: (training)96.50735 (testing) 80.85938\n",
      "[epoch 00458]  loss: (training)0.00679 (testing) 0.00773, accuracy: (training)96.32353 (testing) 81.64062\n",
      "[epoch 00459]  loss: (training)0.00681 (testing) 0.00764, accuracy: (training)96.13971 (testing) 82.42188\n",
      "[epoch 00460]  loss: (training)0.00683 (testing) 0.00767, accuracy: (training)95.86397 (testing) 82.03125\n",
      "[epoch 00461]  loss: (training)0.00683 (testing) 0.00757, accuracy: (training)96.04779 (testing) 82.42188\n",
      "[epoch 00462]  loss: (training)0.00684 (testing) 0.00765, accuracy: (training)95.86397 (testing) 82.42188\n",
      "[epoch 00463]  loss: (training)0.00679 (testing) 0.00754, accuracy: (training)96.13971 (testing) 82.42188\n",
      "[epoch 00464]  loss: (training)0.00683 (testing) 0.0076, accuracy: (training)95.86397 (testing) 82.42188\n",
      "[epoch 00465]  loss: (training)0.00678 (testing) 0.00757, accuracy: (training)96.04779 (testing) 82.42188\n",
      "[epoch 00466]  loss: (training)0.0068 (testing) 0.00757, accuracy: (training)95.95588 (testing) 82.42188\n",
      "[epoch 00467]  loss: (training)0.00678 (testing) 0.00756, accuracy: (training)95.95588 (testing) 82.42188\n",
      "[epoch 00468]  loss: (training)0.00676 (testing) 0.00756, accuracy: (training)96.04779 (testing) 82.42188\n",
      "[epoch 00469]  loss: (training)0.00674 (testing) 0.00753, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00470]  loss: (training)0.00674 (testing) 0.00754, accuracy: (training)96.41544 (testing) 82.42188\n",
      "[epoch 00471]  loss: (training)0.00673 (testing) 0.00752, accuracy: (training)96.32353 (testing) 82.42188\n",
      "[epoch 00472]  loss: (training)0.00673 (testing) 0.00752, accuracy: (training)96.32353 (testing) 82.42188\n",
      "[epoch 00473]  loss: (training)0.00672 (testing) 0.00751, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00474]  loss: (training)0.00672 (testing) 0.00755, accuracy: (training)96.78309 (testing) 82.42188\n",
      "[epoch 00475]  loss: (training)0.00672 (testing) 0.00758, accuracy: (training)96.78309 (testing) 82.42188\n",
      "[epoch 00476]  loss: (training)0.00672 (testing) 0.00756, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00477]  loss: (training)0.00672 (testing) 0.00759, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00478]  loss: (training)0.00671 (testing) 0.00757, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00479]  loss: (training)0.00673 (testing) 0.00762, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00480]  loss: (training)0.00671 (testing) 0.00791, accuracy: (training)96.59926 (testing) 79.29688\n",
      "[epoch 00481]  loss: (training)0.00677 (testing) 0.00864, accuracy: (training)96.32353 (testing) 75.0\n",
      "[epoch 00482]  loss: (training)0.00679 (testing) 0.00788, accuracy: (training)96.23162 (testing) 79.6875\n",
      "[epoch 00483]  loss: (training)0.00676 (testing) 0.00804, accuracy: (training)96.41544 (testing) 79.29688\n",
      "[epoch 00484]  loss: (training)0.00678 (testing) 0.00785, accuracy: (training)96.50735 (testing) 80.07812\n",
      "[epoch 00485]  loss: (training)0.00677 (testing) 0.00782, accuracy: (training)96.50735 (testing) 80.46875\n",
      "[epoch 00486]  loss: (training)0.00676 (testing) 0.0077, accuracy: (training)96.50735 (testing) 82.03125\n",
      "[epoch 00487]  loss: (training)0.00676 (testing) 0.00772, accuracy: (training)96.41544 (testing) 81.64062\n",
      "[epoch 00488]  loss: (training)0.00673 (testing) 0.00766, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00489]  loss: (training)0.00677 (testing) 0.00769, accuracy: (training)96.41544 (testing) 81.64062\n",
      "[epoch 00490]  loss: (training)0.00674 (testing) 0.00759, accuracy: (training)96.50735 (testing) 82.42188\n",
      "[epoch 00491]  loss: (training)0.00676 (testing) 0.00764, accuracy: (training)96.59926 (testing) 82.42188\n",
      "[epoch 00492]  loss: (training)0.00675 (testing) 0.00752, accuracy: (training)96.32353 (testing) 82.8125\n",
      "[epoch 00493]  loss: (training)0.00671 (testing) 0.00755, accuracy: (training)96.78309 (testing) 82.42188\n",
      "[epoch 00494]  loss: (training)0.00672 (testing) 0.0076, accuracy: (training)96.69118 (testing) 82.42188\n",
      "[epoch 00495]  loss: (training)0.00672 (testing) 0.00754, accuracy: (training)96.78309 (testing) 82.8125\n",
      "[epoch 00496]  loss: (training)0.00672 (testing) 0.00754, accuracy: (training)96.78309 (testing) 82.42188\n",
      "[epoch 00497]  loss: (training)0.00673 (testing) 0.00755, accuracy: (training)96.78309 (testing) 82.42188\n",
      "[epoch 00498]  loss: (training)0.00673 (testing) 0.00753, accuracy: (training)96.78309 (testing) 82.8125\n",
      "[epoch 00499]  loss: (training)0.00673 (testing) 0.00754, accuracy: (training)96.69118 (testing) 82.8125\n",
      "[epoch 00500]  loss: (training)0.00672 (testing) 0.00747, accuracy: (training)96.59926 (testing) 82.8125\n",
      "[epoch 00501]  loss: (training)0.00672 (testing) 0.00749, accuracy: (training)96.59926 (testing) 82.8125\n",
      "[epoch 00502]  loss: (training)0.00671 (testing) 0.00745, accuracy: (training)96.59926 (testing) 82.8125\n",
      "[epoch 00503]  loss: (training)0.00671 (testing) 0.00743, accuracy: (training)96.59926 (testing) 82.8125\n",
      "[epoch 00504]  loss: (training)0.00671 (testing) 0.00741, accuracy: (training)96.50735 (testing) 82.8125\n",
      "[epoch 00505]  loss: (training)0.0067 (testing) 0.0074, accuracy: (training)96.78309 (testing) 82.8125\n",
      "[epoch 00506]  loss: (training)0.0067 (testing) 0.00738, accuracy: (training)96.78309 (testing) 83.20312\n",
      "[epoch 00507]  loss: (training)0.00669 (testing) 0.00736, accuracy: (training)96.78309 (testing) 83.20312\n",
      "[epoch 00508]  loss: (training)0.0067 (testing) 0.00734, accuracy: (training)96.59926 (testing) 84.375\n",
      "[epoch 00509]  loss: (training)0.00669 (testing) 0.00734, accuracy: (training)96.69118 (testing) 83.59375\n",
      "[epoch 00510]  loss: (training)0.00668 (testing) 0.00736, accuracy: (training)96.78309 (testing) 83.59375\n",
      "[epoch 00511]  loss: (training)0.00668 (testing) 0.00737, accuracy: (training)96.875 (testing) 83.20312\n",
      "[epoch 00512]  loss: (training)0.00667 (testing) 0.00739, accuracy: (training)96.96691 (testing) 83.20312\n",
      "[epoch 00513]  loss: (training)0.00666 (testing) 0.0074, accuracy: (training)96.96691 (testing) 83.20312\n",
      "[epoch 00514]  loss: (training)0.00666 (testing) 0.0074, accuracy: (training)96.96691 (testing) 83.20312\n",
      "[epoch 00515]  loss: (training)0.00666 (testing) 0.0074, accuracy: (training)96.96691 (testing) 83.20312\n",
      "[epoch 00516]  loss: (training)0.00666 (testing) 0.00737, accuracy: (training)96.78309 (testing) 83.20312\n",
      "[epoch 00517]  loss: (training)0.00668 (testing) 0.00731, accuracy: (training)96.41544 (testing) 84.375\n",
      "[epoch 00518]  loss: (training)0.00667 (testing) 0.00728, accuracy: (training)96.50735 (testing) 84.76562\n",
      "[epoch 00519]  loss: (training)0.00667 (testing) 0.00732, accuracy: (training)96.50735 (testing) 84.375\n",
      "[epoch 00520]  loss: (training)0.00666 (testing) 0.00734, accuracy: (training)96.78309 (testing) 83.59375\n",
      "[epoch 00521]  loss: (training)0.00664 (testing) 0.0074, accuracy: (training)96.78309 (testing) 83.20312\n",
      "[epoch 00522]  loss: (training)0.00664 (testing) 0.00739, accuracy: (training)96.78309 (testing) 83.20312\n",
      "[epoch 00523]  loss: (training)0.00664 (testing) 0.00736, accuracy: (training)96.78309 (testing) 83.59375\n",
      "[epoch 00524]  loss: (training)0.00664 (testing) 0.00733, accuracy: (training)96.78309 (testing) 83.98438\n",
      "[epoch 00525]  loss: (training)0.00663 (testing) 0.00732, accuracy: (training)96.875 (testing) 83.98438\n",
      "[epoch 00526]  loss: (training)0.00662 (testing) 0.00732, accuracy: (training)96.96691 (testing) 83.98438\n",
      "[epoch 00527]  loss: (training)0.00662 (testing) 0.00732, accuracy: (training)96.96691 (testing) 83.98438\n",
      "[epoch 00528]  loss: (training)0.00661 (testing) 0.00733, accuracy: (training)97.05882 (testing) 83.98438\n",
      "[epoch 00529]  loss: (training)0.00661 (testing) 0.00733, accuracy: (training)97.15074 (testing) 83.98438\n",
      "[epoch 00530]  loss: (training)0.00661 (testing) 0.00734, accuracy: (training)97.24265 (testing) 83.59375\n",
      "[epoch 00531]  loss: (training)0.00661 (testing) 0.00734, accuracy: (training)97.24265 (testing) 84.375\n",
      "[epoch 00532]  loss: (training)0.00661 (testing) 0.00735, accuracy: (training)97.24265 (testing) 83.98438\n",
      "[epoch 00533]  loss: (training)0.0066 (testing) 0.00733, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00534]  loss: (training)0.00659 (testing) 0.00734, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00535]  loss: (training)0.0066 (testing) 0.00734, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00536]  loss: (training)0.00659 (testing) 0.00734, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00537]  loss: (training)0.00659 (testing) 0.00734, accuracy: (training)97.24265 (testing) 84.375\n",
      "[epoch 00538]  loss: (training)0.00659 (testing) 0.00735, accuracy: (training)97.24265 (testing) 84.375\n",
      "[epoch 00539]  loss: (training)0.00659 (testing) 0.00735, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00540]  loss: (training)0.0066 (testing) 0.00735, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00541]  loss: (training)0.0066 (testing) 0.00733, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00542]  loss: (training)0.0066 (testing) 0.00733, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00543]  loss: (training)0.00659 (testing) 0.00732, accuracy: (training)97.42647 (testing) 84.375\n",
      "[epoch 00544]  loss: (training)0.00659 (testing) 0.00731, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00545]  loss: (training)0.00659 (testing) 0.0073, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00546]  loss: (training)0.00659 (testing) 0.00729, accuracy: (training)97.33456 (testing) 84.375\n",
      "[epoch 00547]  loss: (training)0.00659 (testing) 0.00729, accuracy: (training)97.33456 (testing) 84.76562\n",
      "[epoch 00548]  loss: (training)0.00658 (testing) 0.00728, accuracy: (training)97.33456 (testing) 84.76562\n",
      "[epoch 00549]  loss: (training)0.00658 (testing) 0.00728, accuracy: (training)97.33456 (testing) 84.76562\n",
      "[epoch 00550]  loss: (training)0.00658 (testing) 0.00727, accuracy: (training)97.33456 (testing) 84.76562\n",
      "[epoch 00551]  loss: (training)0.00657 (testing) 0.00726, accuracy: (training)97.42647 (testing) 84.76562\n",
      "[epoch 00552]  loss: (training)0.00657 (testing) 0.00726, accuracy: (training)97.42647 (testing) 84.76562\n",
      "[epoch 00553]  loss: (training)0.00656 (testing) 0.00725, accuracy: (training)97.42647 (testing) 84.76562\n",
      "[epoch 00554]  loss: (training)0.00656 (testing) 0.00725, accuracy: (training)97.51838 (testing) 84.76562\n",
      "[epoch 00555]  loss: (training)0.00656 (testing) 0.00724, accuracy: (training)97.51838 (testing) 84.76562\n",
      "[epoch 00556]  loss: (training)0.00655 (testing) 0.00723, accuracy: (training)97.51838 (testing) 84.76562\n",
      "[epoch 00557]  loss: (training)0.00655 (testing) 0.00722, accuracy: (training)97.51838 (testing) 84.76562\n",
      "[epoch 00558]  loss: (training)0.00655 (testing) 0.00722, accuracy: (training)97.51838 (testing) 84.76562\n",
      "[epoch 00559]  loss: (training)0.00654 (testing) 0.00721, accuracy: (training)97.61029 (testing) 84.76562\n",
      "[epoch 00560]  loss: (training)0.00654 (testing) 0.00721, accuracy: (training)97.61029 (testing) 84.76562\n",
      "[epoch 00561]  loss: (training)0.00654 (testing) 0.0072, accuracy: (training)97.70221 (testing) 84.76562\n",
      "[epoch 00562]  loss: (training)0.00654 (testing) 0.0072, accuracy: (training)97.70221 (testing) 84.76562\n",
      "[epoch 00563]  loss: (training)0.00653 (testing) 0.00719, accuracy: (training)97.70221 (testing) 84.76562\n",
      "[epoch 00564]  loss: (training)0.00653 (testing) 0.00719, accuracy: (training)97.70221 (testing) 84.76562\n",
      "[epoch 00565]  loss: (training)0.00653 (testing) 0.00718, accuracy: (training)97.70221 (testing) 84.76562\n",
      "[epoch 00566]  loss: (training)0.00653 (testing) 0.00718, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00567]  loss: (training)0.00653 (testing) 0.00717, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00568]  loss: (training)0.00652 (testing) 0.00717, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00569]  loss: (training)0.00652 (testing) 0.00716, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00570]  loss: (training)0.00652 (testing) 0.00716, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00571]  loss: (training)0.00652 (testing) 0.00715, accuracy: (training)97.79412 (testing) 84.76562\n",
      "[epoch 00572]  loss: (training)0.00652 (testing) 0.00715, accuracy: (training)97.88603 (testing) 84.76562\n",
      "[epoch 00573]  loss: (training)0.00652 (testing) 0.00714, accuracy: (training)97.88603 (testing) 84.76562\n",
      "[epoch 00574]  loss: (training)0.00652 (testing) 0.00714, accuracy: (training)97.88603 (testing) 84.76562\n",
      "[epoch 00575]  loss: (training)0.00651 (testing) 0.00713, accuracy: (training)97.88603 (testing) 85.15625\n",
      "[epoch 00576]  loss: (training)0.00651 (testing) 0.00713, accuracy: (training)97.88603 (testing) 85.15625\n",
      "[epoch 00577]  loss: (training)0.00651 (testing) 0.00712, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00578]  loss: (training)0.00651 (testing) 0.00712, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00579]  loss: (training)0.00651 (testing) 0.00711, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00580]  loss: (training)0.00651 (testing) 0.00711, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00581]  loss: (training)0.0065 (testing) 0.00711, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00582]  loss: (training)0.0065 (testing) 0.0071, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00583]  loss: (training)0.0065 (testing) 0.0071, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00584]  loss: (training)0.0065 (testing) 0.0071, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00585]  loss: (training)0.0065 (testing) 0.00709, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00586]  loss: (training)0.0065 (testing) 0.00709, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00587]  loss: (training)0.00649 (testing) 0.00709, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00588]  loss: (training)0.00649 (testing) 0.00708, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00589]  loss: (training)0.00649 (testing) 0.00708, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00590]  loss: (training)0.00649 (testing) 0.00708, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00591]  loss: (training)0.00649 (testing) 0.00707, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00592]  loss: (training)0.00649 (testing) 0.00707, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00593]  loss: (training)0.00648 (testing) 0.00707, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00594]  loss: (training)0.00648 (testing) 0.00706, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00595]  loss: (training)0.00648 (testing) 0.00706, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00596]  loss: (training)0.00648 (testing) 0.00706, accuracy: (training)97.88603 (testing) 85.54688\n",
      "[epoch 00597]  loss: (training)0.00648 (testing) 0.00706, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00598]  loss: (training)0.00648 (testing) 0.00706, accuracy: (training)97.88603 (testing) 85.9375\n",
      "[epoch 00599]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00600]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00601]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00602]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00603]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00604]  loss: (training)0.00647 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00605]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00606]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.88603 (testing) 86.32812\n",
      "[epoch 00607]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00608]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00609]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00610]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00611]  loss: (training)0.00646 (testing) 0.00705, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00612]  loss: (training)0.00646 (testing) 0.00706, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00613]  loss: (training)0.00646 (testing) 0.00706, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00614]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00615]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)97.97794 (testing) 86.32812\n",
      "[epoch 00616]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00617]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00618]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00619]  loss: (training)0.00645 (testing) 0.00706, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00620]  loss: (training)0.00645 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00621]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00622]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00623]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00624]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00625]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00626]  loss: (training)0.00644 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00627]  loss: (training)0.00643 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00628]  loss: (training)0.00643 (testing) 0.00705, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00629]  loss: (training)0.00643 (testing) 0.00704, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00630]  loss: (training)0.00643 (testing) 0.00704, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00631]  loss: (training)0.00643 (testing) 0.00704, accuracy: (training)98.06985 (testing) 86.32812\n",
      "[epoch 00632]  loss: (training)0.00642 (testing) 0.00704, accuracy: (training)98.16176 (testing) 86.32812\n",
      "[epoch 00633]  loss: (training)0.00642 (testing) 0.00703, accuracy: (training)98.16176 (testing) 86.32812\n",
      "[epoch 00634]  loss: (training)0.00642 (testing) 0.00703, accuracy: (training)98.16176 (testing) 86.32812\n",
      "[epoch 00635]  loss: (training)0.00642 (testing) 0.00702, accuracy: (training)98.16176 (testing) 86.32812\n",
      "[epoch 00636]  loss: (training)0.00642 (testing) 0.00702, accuracy: (training)98.16176 (testing) 86.32812\n",
      "[epoch 00637]  loss: (training)0.00641 (testing) 0.00701, accuracy: (training)98.16176 (testing) 86.71875\n",
      "[epoch 00638]  loss: (training)0.00641 (testing) 0.00701, accuracy: (training)98.16176 (testing) 86.71875\n",
      "[epoch 00639]  loss: (training)0.00641 (testing) 0.007, accuracy: (training)98.16176 (testing) 86.71875\n",
      "[epoch 00640]  loss: (training)0.00641 (testing) 0.007, accuracy: (training)98.25368 (testing) 86.71875\n",
      "[epoch 00641]  loss: (training)0.0064 (testing) 0.00699, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00642]  loss: (training)0.0064 (testing) 0.00698, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00643]  loss: (training)0.0064 (testing) 0.00698, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00644]  loss: (training)0.0064 (testing) 0.00697, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00645]  loss: (training)0.0064 (testing) 0.00697, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00646]  loss: (training)0.00639 (testing) 0.00696, accuracy: (training)98.34559 (testing) 86.71875\n",
      "[epoch 00647]  loss: (training)0.00639 (testing) 0.00696, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00648]  loss: (training)0.00639 (testing) 0.00695, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00649]  loss: (training)0.00639 (testing) 0.00695, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00650]  loss: (training)0.00639 (testing) 0.00694, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00651]  loss: (training)0.00639 (testing) 0.00693, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00652]  loss: (training)0.00638 (testing) 0.00693, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00653]  loss: (training)0.00638 (testing) 0.00692, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00654]  loss: (training)0.00638 (testing) 0.00692, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00655]  loss: (training)0.00638 (testing) 0.00691, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00656]  loss: (training)0.00638 (testing) 0.00691, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00657]  loss: (training)0.00638 (testing) 0.0069, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00658]  loss: (training)0.00637 (testing) 0.0069, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00659]  loss: (training)0.00637 (testing) 0.0069, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00660]  loss: (training)0.00637 (testing) 0.00689, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00661]  loss: (training)0.00637 (testing) 0.00689, accuracy: (training)98.4375 (testing) 87.10938\n",
      "[epoch 00662]  loss: (training)0.00637 (testing) 0.00688, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00663]  loss: (training)0.00636 (testing) 0.00688, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00664]  loss: (training)0.00636 (testing) 0.00687, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00665]  loss: (training)0.00636 (testing) 0.00687, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00666]  loss: (training)0.00636 (testing) 0.00687, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00667]  loss: (training)0.00636 (testing) 0.00686, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00668]  loss: (training)0.00636 (testing) 0.00686, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00669]  loss: (training)0.00636 (testing) 0.00686, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00670]  loss: (training)0.00635 (testing) 0.00686, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00671]  loss: (training)0.00635 (testing) 0.00685, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00672]  loss: (training)0.00635 (testing) 0.00685, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00673]  loss: (training)0.00635 (testing) 0.00685, accuracy: (training)98.4375 (testing) 86.71875\n",
      "[epoch 00674]  loss: (training)0.00635 (testing) 0.00684, accuracy: (training)98.52941 (testing) 86.71875\n",
      "[epoch 00675]  loss: (training)0.00635 (testing) 0.00684, accuracy: (training)98.52941 (testing) 87.10938\n",
      "[epoch 00676]  loss: (training)0.00635 (testing) 0.00684, accuracy: (training)98.62132 (testing) 87.10938\n",
      "[epoch 00677]  loss: (training)0.00634 (testing) 0.00683, accuracy: (training)98.62132 (testing) 87.10938\n",
      "[epoch 00678]  loss: (training)0.00634 (testing) 0.00683, accuracy: (training)98.62132 (testing) 87.5\n",
      "[epoch 00679]  loss: (training)0.00634 (testing) 0.00682, accuracy: (training)98.62132 (testing) 87.5\n",
      "[epoch 00680]  loss: (training)0.00634 (testing) 0.00682, accuracy: (training)98.62132 (testing) 87.5\n",
      "[epoch 00681]  loss: (training)0.00634 (testing) 0.00681, accuracy: (training)98.62132 (testing) 87.5\n",
      "[epoch 00682]  loss: (training)0.00634 (testing) 0.00681, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00683]  loss: (training)0.00634 (testing) 0.0068, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00684]  loss: (training)0.00633 (testing) 0.0068, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00685]  loss: (training)0.00633 (testing) 0.00679, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00686]  loss: (training)0.00633 (testing) 0.00679, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00687]  loss: (training)0.00633 (testing) 0.00678, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00688]  loss: (training)0.00633 (testing) 0.00678, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00689]  loss: (training)0.00632 (testing) 0.00677, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00690]  loss: (training)0.00632 (testing) 0.00677, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00691]  loss: (training)0.00632 (testing) 0.00676, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00692]  loss: (training)0.00632 (testing) 0.00676, accuracy: (training)98.62132 (testing) 87.89062\n",
      "[epoch 00693]  loss: (training)0.00632 (testing) 0.00675, accuracy: (training)98.71324 (testing) 87.89062\n",
      "[epoch 00694]  loss: (training)0.00631 (testing) 0.00675, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00695]  loss: (training)0.00631 (testing) 0.00674, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00696]  loss: (training)0.00631 (testing) 0.00674, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00697]  loss: (training)0.00631 (testing) 0.00673, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00698]  loss: (training)0.00631 (testing) 0.00673, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00699]  loss: (training)0.00631 (testing) 0.00673, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00700]  loss: (training)0.00631 (testing) 0.00672, accuracy: (training)98.71324 (testing) 88.28125\n",
      "[epoch 00701]  loss: (training)0.00631 (testing) 0.00672, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00702]  loss: (training)0.0063 (testing) 0.00671, accuracy: (training)98.80515 (testing) 88.67188\n",
      "[epoch 00703]  loss: (training)0.0063 (testing) 0.00671, accuracy: (training)98.80515 (testing) 88.67188\n",
      "[epoch 00704]  loss: (training)0.0063 (testing) 0.00671, accuracy: (training)98.80515 (testing) 88.67188\n",
      "[epoch 00705]  loss: (training)0.0063 (testing) 0.0067, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00706]  loss: (training)0.0063 (testing) 0.0067, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00707]  loss: (training)0.0063 (testing) 0.0067, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00708]  loss: (training)0.0063 (testing) 0.00669, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00709]  loss: (training)0.00629 (testing) 0.00669, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00710]  loss: (training)0.00629 (testing) 0.00669, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00711]  loss: (training)0.00629 (testing) 0.00668, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00712]  loss: (training)0.00629 (testing) 0.00668, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00713]  loss: (training)0.00629 (testing) 0.00668, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00714]  loss: (training)0.00629 (testing) 0.00668, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00715]  loss: (training)0.00629 (testing) 0.00667, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00716]  loss: (training)0.00628 (testing) 0.00667, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00717]  loss: (training)0.00628 (testing) 0.00667, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00718]  loss: (training)0.00628 (testing) 0.00667, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00719]  loss: (training)0.00628 (testing) 0.00666, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00720]  loss: (training)0.00628 (testing) 0.00666, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00721]  loss: (training)0.00627 (testing) 0.00666, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00722]  loss: (training)0.00627 (testing) 0.00666, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00723]  loss: (training)0.00627 (testing) 0.00665, accuracy: (training)98.80515 (testing) 88.28125\n",
      "[epoch 00724]  loss: (training)0.00627 (testing) 0.00665, accuracy: (training)98.89706 (testing) 88.28125\n",
      "[epoch 00725]  loss: (training)0.00627 (testing) 0.00665, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00726]  loss: (training)0.00627 (testing) 0.00664, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00727]  loss: (training)0.00627 (testing) 0.00664, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00728]  loss: (training)0.00626 (testing) 0.00664, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00729]  loss: (training)0.00626 (testing) 0.00664, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00730]  loss: (training)0.00626 (testing) 0.00663, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00731]  loss: (training)0.00626 (testing) 0.00663, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00732]  loss: (training)0.00626 (testing) 0.00663, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00733]  loss: (training)0.00626 (testing) 0.00662, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00734]  loss: (training)0.00626 (testing) 0.00662, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00735]  loss: (training)0.00626 (testing) 0.00662, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00736]  loss: (training)0.00625 (testing) 0.00661, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00737]  loss: (training)0.00625 (testing) 0.00661, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00738]  loss: (training)0.00625 (testing) 0.00661, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00739]  loss: (training)0.00625 (testing) 0.0066, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00740]  loss: (training)0.00625 (testing) 0.0066, accuracy: (training)98.98897 (testing) 87.89062\n",
      "[epoch 00741]  loss: (training)0.00625 (testing) 0.0066, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00742]  loss: (training)0.00624 (testing) 0.00659, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00743]  loss: (training)0.00624 (testing) 0.00659, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00744]  loss: (training)0.00624 (testing) 0.00658, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00745]  loss: (training)0.00624 (testing) 0.00658, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00746]  loss: (training)0.00624 (testing) 0.00658, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00747]  loss: (training)0.00624 (testing) 0.00657, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00748]  loss: (training)0.00623 (testing) 0.00657, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00749]  loss: (training)0.00623 (testing) 0.00657, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00750]  loss: (training)0.00623 (testing) 0.00656, accuracy: (training)98.98897 (testing) 88.28125\n",
      "[epoch 00751]  loss: (training)0.00623 (testing) 0.00656, accuracy: (training)98.98897 (testing) 88.67188\n",
      "[epoch 00752]  loss: (training)0.00623 (testing) 0.00656, accuracy: (training)99.08088 (testing) 88.67188\n",
      "[epoch 00753]  loss: (training)0.00623 (testing) 0.00655, accuracy: (training)99.08088 (testing) 88.67188\n",
      "[epoch 00754]  loss: (training)0.00623 (testing) 0.00655, accuracy: (training)99.08088 (testing) 88.67188\n",
      "[epoch 00755]  loss: (training)0.00622 (testing) 0.00655, accuracy: (training)99.17279 (testing) 88.67188\n",
      "[epoch 00756]  loss: (training)0.00622 (testing) 0.00655, accuracy: (training)99.17279 (testing) 89.0625\n",
      "[epoch 00757]  loss: (training)0.00622 (testing) 0.00654, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00758]  loss: (training)0.00622 (testing) 0.00654, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00759]  loss: (training)0.00622 (testing) 0.00654, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00760]  loss: (training)0.00622 (testing) 0.00654, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00761]  loss: (training)0.00622 (testing) 0.00654, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00762]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.17279 (testing) 89.45312\n",
      "[epoch 00763]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00764]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00765]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00766]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00767]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00768]  loss: (training)0.00622 (testing) 0.00653, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00769]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00770]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00771]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00772]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00773]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00774]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00775]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00776]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00777]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00778]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00779]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00780]  loss: (training)0.00621 (testing) 0.00652, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00781]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00782]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00783]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00784]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00785]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00786]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00787]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00788]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00789]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00790]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00791]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00792]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00793]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00794]  loss: (training)0.00621 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00795]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00796]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00797]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00798]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00799]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00800]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00801]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00802]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.45312\n",
      "[epoch 00803]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00804]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00805]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00806]  loss: (training)0.0062 (testing) 0.00651, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00807]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00808]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00809]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00810]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00811]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00812]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00813]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00814]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00815]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00816]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00817]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00818]  loss: (training)0.0062 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00819]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00820]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00821]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00822]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00823]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.26471 (testing) 89.84375\n",
      "[epoch 00824]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00825]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00826]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00827]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00828]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00829]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00830]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00831]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00832]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00833]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00834]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00835]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00836]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00837]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00838]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00839]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00840]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00841]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00842]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00843]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00844]  loss: (training)0.00619 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00845]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00846]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00847]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00848]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00849]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00850]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00851]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00852]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00853]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00854]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00855]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00856]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00857]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00858]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00859]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00860]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00861]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00862]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00863]  loss: (training)0.00618 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00864]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00865]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00866]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00867]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00868]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00869]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 90.23438\n",
      "[epoch 00870]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00871]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 90.23438\n",
      "[epoch 00872]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 90.23438\n",
      "[epoch 00873]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 89.84375\n",
      "[epoch 00874]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 90.23438\n",
      "[epoch 00875]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.35662 (testing) 90.23438\n",
      "[epoch 00876]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00877]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00878]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00879]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00880]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00881]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00882]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00883]  loss: (training)0.00617 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00884]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00885]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00886]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00887]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 89.84375\n",
      "[epoch 00888]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00889]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00890]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00891]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 89.84375\n",
      "[epoch 00892]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00893]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00894]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00895]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00896]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00897]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00898]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00899]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00900]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00901]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00902]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00903]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00904]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00905]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00906]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.44853 (testing) 90.23438\n",
      "[epoch 00907]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.54044 (testing) 90.23438\n",
      "[epoch 00908]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.54044 (testing) 90.23438\n",
      "[epoch 00909]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00910]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00911]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00912]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00913]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00914]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00915]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00916]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00917]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00918]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00919]  loss: (training)0.00616 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00920]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00921]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00922]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00923]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00924]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00925]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00926]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00927]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00928]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00929]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00930]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00931]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00932]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00933]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00934]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00935]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00936]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00937]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00938]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00939]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00940]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00941]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00942]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00943]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00944]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00945]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00946]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00947]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00948]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00949]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00950]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00951]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00952]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00953]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00954]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00955]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00956]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00957]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00958]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00959]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00960]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00961]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00962]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00963]  loss: (training)0.00615 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00964]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00965]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00966]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00967]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00968]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00969]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00970]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00971]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00972]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00973]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00974]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00975]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00976]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.63235 (testing) 90.23438\n",
      "[epoch 00977]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00978]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00979]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00980]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00981]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00982]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00983]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00984]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00985]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00986]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00987]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00988]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00989]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00990]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00991]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00992]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00993]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00994]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00995]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00996]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00997]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00998]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n",
      "[epoch 00999]  loss: (training)0.00614 (testing) 0.0065, accuracy: (training)99.72426 (testing) 90.23438\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "loss_train_mean = list()\n",
    "loss_train_std = list()\n",
    "acc_train_mean = list()\n",
    "acc_train_std = list()\n",
    "loss_test = list()\n",
    "accuracy_test = list()\n",
    "\n",
    "for e in range(epoch):\n",
    "        \n",
    "    result_train    = train()\n",
    "    result_test     = test()\n",
    "    \n",
    "    result_loss_train_mean = result_train['loss_train_mean']\n",
    "    result_loss_train_std =  result_train['loss_train_std']\n",
    "    result_acc_train_mean =  result_train['acc_train_mean']\n",
    "    result_acc_train_std =  result_train['acc_train_std']\n",
    "    result_loss_test =       result_test['loss_test']\n",
    "    result_accuracy_test =   result_test['accuracy_test']\n",
    "\n",
    "    loss_train_mean.append( result_loss_train_mean)\n",
    "    loss_train_std.append(  result_loss_train_std)\n",
    "    acc_train_mean.append(  result_acc_train_mean)\n",
    "    acc_train_std.append(  result_acc_train_std)\n",
    "    loss_test.append(       result_loss_test)\n",
    "    accuracy_test.append(   result_accuracy_test)\n",
    "    \n",
    "    epoch_str = '[epoch '+ '{:05d}'.format(e)+ ']  '\n",
    "    epoch_str += 'loss: '+ '(training)' + str(round(result_loss_train_mean, 5))+ ' (testing) ' + str(round(result_loss_test, 5)) + ', '\n",
    "    epoch_str += 'accuracy: '+ '(training)' + str(round(result_acc_train_mean, 5))+ ' (testing) ' + str(round(result_accuracy_test, 5))\n",
    "    print(epoch_str)\n",
    "    \n",
    "    if e > epoch - 100 and accuracy_test[-1] > accuracy_test[-2] and accuracy_test[-1] > accuracy_test[-3]:\n",
    "        break;\n",
    "    \n",
    "    #adaptive learning rate\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = g['lr'] * 0.9989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1Ri6ILMQZtU"
   },
   "source": [
    "##### plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIPGvOdYQZtU"
   },
   "outputs": [],
   "source": [
    "def print_history_graph(loss_history, loss_train_std, acc_history, acc_train_std, vloss_history, vacc_history):\n",
    "    #plt.plot(acc_history, color='#ff0000', label='Train Accuracy')\n",
    "    plt.errorbar(list(range(len(acc_history))), acc_history, yerr=acc_train_std, ecolor = '#ffcccc', color='#ff0000', label='Train Accuracy')\n",
    "    plt.plot(vacc_history, color='#0000ff', label='Validation Accuracy')\n",
    "    plt.legend(['Validation Accuracy', 'Train Accuracy'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(loss_history, color='#ff0000', label='Train Loss')\n",
    "    plt.errorbar(list(range(len(loss_history))), loss_history, yerr=loss_train_std, ecolor = '#ffcccc', color='#ff0000', label='Train Loss')\n",
    "    plt.plot(vloss_history, color='#0000ff', label='Validation Loss')\n",
    "    plt.legend(['Validation Loss','Train Loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    nx, ny = 3, 3\n",
    "    data = (('dataset', 'loss', 'accuracy'), ('train', str(np.round(loss_history[-1],4)), str(np.round(acc_history[-1],4))), ('validation', str(np.round(vloss_history[-1],4)), str(np.round(vacc_history[-1],4))))\n",
    "    pl.figure()\n",
    "    tb = pl.table(cellText=data, loc=(0,0), cellLoc='center')\n",
    "\n",
    "    tc = tb.properties()['child_artists']\n",
    "    for cell in tc: \n",
    "        cell.set_height(1/ny)\n",
    "        cell.set_width(1/nx)\n",
    "\n",
    "    ax = pl.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "id": "wrXL9tkiQZtW",
    "outputId": "f075395c-07aa-491d-e28b-b6e0e2cbb165"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGgCUUipIaQBQSqkgpQFAKaWC1P8DGgcGM1guG/EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLbglAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_history_graph(loss_train_mean, loss_train_std, acc_train_mean, acc_train_std, loss_test, accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment07.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
